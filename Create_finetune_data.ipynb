{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cae8ab7",
   "metadata": {},
   "source": [
    "## Create a sample set to generate a dataset for fine tuning.\n",
    "\n",
    "First load the FACTors data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52b82330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original rows: 118112\n",
      "Articles with multiple claims: 12\n",
      "Rows after removing duplicates: 117981\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "factors_df = pd.read_csv(\"Data/FACTors.csv\")\n",
    "\n",
    "# Identify article_ids that occur only once\n",
    "article_counts = factors_df['article_id'].value_counts()\n",
    "duplicate_article_ids = article_counts[article_counts > 1]\n",
    "unique_article_ids = article_counts[article_counts == 1].index\n",
    "\n",
    "# Filter the DataFrame to keep only unique article_ids\n",
    "clean_factors_df = factors_df[factors_df['article_id'].isin(unique_article_ids)]\n",
    "\n",
    "# Confirm removal\n",
    "print(f\"Original rows: {len(factors_df)}\")\n",
    "print(f\"Articles with multiple claims: {len(duplicate_article_ids)}\")\n",
    "print(f\"Rows after removing duplicates: {len(clean_factors_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1b081f",
   "metadata": {},
   "source": [
    "## Build a dataset with claims and factchecked answers\n",
    "Retrieve first a sample of 1000 claims and fact checked articles, make sure to divide the verdicts equally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e077d21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a subset of the largest fact checking organisations\n",
    "factors_sub_df=clean_factors_df[clean_factors_df[\"organisation\"].isin([\"PolitiFact\", \"AFP Fact Check\", \"Snopes\", \"WebQoof\", \"FactCheck.org\"])]\n",
    "factors_sample_df= factors_sub_df.sample(n=3000, random_state=12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f0c315",
   "metadata": {},
   "source": [
    "Retrieve the full articles fromt the url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ed05a6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>date_published</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>81368</th>\n",
       "      <td>\"Arizona officials caught changing ballots, ha...</td>\n",
       "      <td>2024-11-12T00:00:00</td>\n",
       "      <td>https://www.politifact.com/factchecks/2024/nov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90708</th>\n",
       "      <td>The Yeti snow monster from Disneyland's iconic...</td>\n",
       "      <td>2020-11-30T11:45:24</td>\n",
       "      <td>https://www.snopes.com/fact-check/disney-yeti/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67021</th>\n",
       "      <td>\"I can tell you that the enhanced interrogatio...</td>\n",
       "      <td>2016-05-24T00:00:00</td>\n",
       "      <td>https://www.politifact.com/factchecks/2016/may...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10591</th>\n",
       "      <td>Nigerian election tribunal witness goes on the...</td>\n",
       "      <td>2023-07-10T11:38:00</td>\n",
       "      <td>https://factcheck.afp.com/doc.afp.com.33NE7Y8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75871</th>\n",
       "      <td>\"We essentially repealed Obamacare because we ...</td>\n",
       "      <td>2017-12-21T00:00:00</td>\n",
       "      <td>https://www.politifact.com/factchecks/2017/dec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71583</th>\n",
       "      <td>President Obama plans to \"impose a tax of at l...</td>\n",
       "      <td>2011-11-22T00:00:00</td>\n",
       "      <td>https://www.politifact.com/factchecks/2011/nov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75185</th>\n",
       "      <td>\"Almost half a million people are still eligib...</td>\n",
       "      <td>2016-08-22T00:00:00</td>\n",
       "      <td>https://www.politifact.com/factchecks/2016/aug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91346</th>\n",
       "      <td>Two 'racist' Black teenagers shot and killed a...</td>\n",
       "      <td>2020-07-07T08:28:45</td>\n",
       "      <td>https://www.snopes.com/fact-check/thugs-shoot-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71546</th>\n",
       "      <td>Says Barack Obama had \"huge majorities\" in Con...</td>\n",
       "      <td>2011-12-04T00:00:00</td>\n",
       "      <td>https://www.politifact.com/factchecks/2011/dec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91280</th>\n",
       "      <td>Walter \"Blackie\" Wetzel, a former leader of th...</td>\n",
       "      <td>2020-07-16T14:47:57</td>\n",
       "      <td>https://www.snopes.com/fact-check/walter-wetze...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   claim       date_published  \\\n",
       "81368  \"Arizona officials caught changing ballots, ha...  2024-11-12T00:00:00   \n",
       "90708  The Yeti snow monster from Disneyland's iconic...  2020-11-30T11:45:24   \n",
       "67021  \"I can tell you that the enhanced interrogatio...  2016-05-24T00:00:00   \n",
       "10591  Nigerian election tribunal witness goes on the...  2023-07-10T11:38:00   \n",
       "75871  \"We essentially repealed Obamacare because we ...  2017-12-21T00:00:00   \n",
       "71583  President Obama plans to \"impose a tax of at l...  2011-11-22T00:00:00   \n",
       "75185  \"Almost half a million people are still eligib...  2016-08-22T00:00:00   \n",
       "91346  Two 'racist' Black teenagers shot and killed a...  2020-07-07T08:28:45   \n",
       "71546  Says Barack Obama had \"huge majorities\" in Con...  2011-12-04T00:00:00   \n",
       "91280  Walter \"Blackie\" Wetzel, a former leader of th...  2020-07-16T14:47:57   \n",
       "\n",
       "                                                     url  \n",
       "81368  https://www.politifact.com/factchecks/2024/nov...  \n",
       "90708     https://www.snopes.com/fact-check/disney-yeti/  \n",
       "67021  https://www.politifact.com/factchecks/2016/may...  \n",
       "10591      https://factcheck.afp.com/doc.afp.com.33NE7Y8  \n",
       "75871  https://www.politifact.com/factchecks/2017/dec...  \n",
       "71583  https://www.politifact.com/factchecks/2011/nov...  \n",
       "75185  https://www.politifact.com/factchecks/2016/aug...  \n",
       "91346  https://www.snopes.com/fact-check/thugs-shoot-...  \n",
       "71546  https://www.politifact.com/factchecks/2011/dec...  \n",
       "91280  https://www.snopes.com/fact-check/walter-wetze...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factors_sample_df=factors_sample_df[['claim','date_published','url']]\n",
    "factors_sample_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55203dc",
   "metadata": {},
   "source": [
    "## Generate a summary and questions\n",
    "Retrieve information and create a summary as done in the original workflow of the assistant for these 1000 claims.\n",
    "\n",
    "GPT5 and GPT OSS 120GB were compared, Since GPT OSS was much faster (53 seconds versus 24,5 min) and produced good results. This was the choice.\n",
    "- https://artificialanalysis.ai/models/gpt-oss-120b/providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fc24b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_information_prompt = \"\"\"\n",
    "### Role\n",
    "You are a neutral, guiding assistant that helps students through the fact-checking process step by step. \n",
    "In this step your are tasked with extracting detailed information about a claim to determine its checkability.\n",
    "\n",
    "### Claim\n",
    "{claim}\n",
    "\n",
    "The claim has already been fact-checked and the outcome was published on this date:\n",
    "### Date published\n",
    "{date_published}\n",
    "\n",
    "### Steps\n",
    "1. Identify the subject.\n",
    "2. Determine if the claim is *quantitative*. \n",
    "3. Assess precision: \"precise\", \"vague\", or \"absolute (100%)\". \n",
    "4. Identify what the claim is *based on* (e.g., \"survey …\", \"official statistics\"). \n",
    "5. Identify the geography and time period mentioned in the claim, if provided. You may assume that the date_published occurs shortly after the claim was made.\n",
    "6. Identify *alerts/warnings*: unclear subject, qualitative claim, vague quantitative claim, geography missing, time period missing, methodological details absent. \n",
    "Don't mention an alert when the information is present.\n",
    "7. Summarize concisely* what is currently known about the claim.\n",
    "   - Include: the information found in the first 5 steps such as subject, type (quantitative/qualitative), precision, basis, and uncertainties.\n",
    "   - Mention any active alerts or missing information.\n",
    "\n",
    "Keep your tone neutral and analytical.\n",
    "\n",
    "### Output Format\n",
    "Return a single JSON object with exactly these fields:\n",
    "\n",
    "- \"alerts\": array of strings. Each alert as a short string; use [] if none.\n",
    "- \"summary\": string. A concise summary of the claim and its checkability status.\n",
    "\n",
    "The response must be valid JSON and contain **only** this JSON object, with no extra text before or after it.\n",
    "\n",
    "### Examples\n",
    "Example A (qualitative):\n",
    "{{\n",
    "  \"alerts\": [\"qualitative claim\", \"methodological details absent\", \"geography present\", \"time period present\"],\n",
    "  \"summary\": \"A qualitative claim about a specific legal event; methodology implied but not fully detailed.\"\n",
    "}}\n",
    "\n",
    "Example B (quantitative but vague):\n",
    "{{\n",
    "  \"alerts\": [\"vague quantitative claim\", \"time period missing\", \"source/methodology missing\", \"geography: EU (present)\"],\n",
    "  \"summary\": \"A quantitative claim lacking precision and methodological details; several key elements are missing for checkability.\"\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9ac2a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_information_prompt_url = \"\"\"\n",
    "### Role\n",
    "You are a neutral, guiding assistant that helps students through the fact-checking process step by step. \n",
    "In this step your are tasked with extracting detailed information about a claim to determine its checkability.\n",
    "\n",
    "### Claim\n",
    "{claim}\n",
    "\n",
    "The claim has already been fact-checked, below the article and date published:\n",
    "### Fact-checked article\n",
    "{date_published}\n",
    "\n",
    "<Article>\n",
    "{article_text}\n",
    "</Article>\n",
    "\n",
    "### Steps\n",
    "1. Identify the subject.\n",
    "2. Determine if the claim is *quantitative*. \n",
    "3. Assess precision: \"precise\", \"vague\", or \"absolute (100%)\". \n",
    "4. Identify what the claim is *based on* (e.g., \"survey …\", \"official statistics\"). \n",
    "5. Identify the geography and time period mentioned in the claim, if provided. You may assume that the date_published occurs shortly after the claim was made.\n",
    "6. Identify *alerts/warnings*: unclear subject, qualitative claim, vague quantitative claim, geography missing, time period missing, methodological details absent. \n",
    "Don't mention an alert when the information is present.\n",
    "7. Summarize concisely* what is currently known about the claim, take into account the information in the *Article*.\n",
    "   - Include: the information found in the first 5 steps such as subject, type (quantitative/qualitative), precision, basis, and uncertainties.\n",
    "   - Check the content of the *Article* for missing information, but don't mention a *verdict* (e.g. True or False) in the summary.\n",
    "   - Mention any active alerts or missing information, that could not be found in *Article*.\n",
    "\n",
    "Keep your tone neutral and analytical.\n",
    "\n",
    "### Output Format\n",
    "Return a single JSON object with exactly these fields:\n",
    "\n",
    "- \"alerts\": array of strings. Each alert as a short string; use [] if none.\n",
    "- \"summary\": string. A concise summary of the claim and its checkability status.\n",
    "\n",
    "The response must be valid JSON and contain **only** this JSON object, with no extra text before or after it.\n",
    "\n",
    "### Examples\n",
    "Example A (qualitative):\n",
    "{{\n",
    "  \"alerts\": [\"qualitative claim\", \"methodological details absent\", \"geography present\", \"time period present\"],\n",
    "  \"summary\": \"A qualitative claim about a specific legal event; methodology implied but not fully detailed.\"\n",
    "}}\n",
    "\n",
    "Example B (quantitative but vague):\n",
    "{{\n",
    "  \"alerts\": [\"vague quantitative claim\", \"time period missing\", \"source/methodology missing\", \"geography: EU (present)\"],\n",
    "  \"summary\": \"A quantitative claim lacking precision and methodological details; several key elements are missing for checkability.\"\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e9884b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "socratic_questions_prompt = \"\"\"You are given a fact-check claim and a summary of what is known about this claim. \n",
    "Your task is to generate 4 Socratic questions that probe the summary up until now. The goal is to challenge the reasoning, \n",
    "surface blind spots, and encourage deeper reflection, not to accept the summary at face value. \n",
    "\n",
    "### Claim\n",
    "{claim}\n",
    "{summary}\n",
    "\n",
    "<Alerts>\n",
    "{alerts}\n",
    "</Alerts>\n",
    "\n",
    "\n",
    "Since the output  will be used to finetune an LLM that critiques the reasoning of a fact-checking model, ensure that your questions reflect the following principles:\n",
    "- Factuality – Do the claims rely on verifiable evidence? Could missing or weak evidence be questioned?\n",
    "- Objectivity – Is the reasoning neutral, or does it show bias? How could the framing be challenged?\n",
    "- Fairness – Are multiple perspectives considered? Is the reasoning applied consistently?\n",
    "- Transparency – Is the summary clear about its sources and reasoning steps? What is hidden or assumed?\n",
    "- Hallucinations – Does the summary risk introducing unsupported or invented information?\n",
    "- Strategies & Alternatives – Are there other ways to frame, investigate, or reason about the claim?\n",
    "\n",
    "When writing questions, draw from the following categories of Socratic questioning. Use them as inspiration to diversify your 4 questions \n",
    "(do not stick to just one category):\n",
    "\n",
    "Purpose – probe the aim or agenda.\n",
    "- What is your purpose right now?\n",
    "- Why are you writing this?\n",
    "- What do you want to persuade them of?\n",
    "- What is our central aim or task in this line of thought?\n",
    "\n",
    "Questions – probe the underlying questions.\n",
    "- I am not sure exactly what question you are raising. Could you explain it?\n",
    "- Is this question the best one to focus on, or is there a more pressing one?\n",
    "- What questions might we be failing to ask that we should be asking?\n",
    "\n",
    "Information – probe the evidence or data.\n",
    "- On what information are you basing that comment?\n",
    "- How do we know this information is accurate? How could we verify it?\n",
    "- Have we failed to consider any information or data we need to consider?\n",
    "\n",
    "Inferences & Conclusions – probe how the conclusion was drawn.\n",
    "- How did you reach that conclusion?\n",
    "- Could you explain your reasoning?\n",
    "- Is there an alternative plausible conclusion?\n",
    "\n",
    "Concepts & Ideas – probe key ideas being applied.\n",
    "- What is the main idea you are using in your reasoning?\n",
    "- Are we using the appropriate concept, or do we need to reconceptualize the problem?\n",
    "- Do we need more facts, or do we need to rethink how we are labeling the facts?\n",
    "\n",
    "Assumptions – probe what is taken for granted.\n",
    "- What exactly are you taking for granted here?\n",
    "- Why are you assuming that? Shouldn’t we rather assume that…?\n",
    "- What alternative assumptions might we make?\n",
    "\n",
    "Implications & Consequences – probe what follows.\n",
    "- What are you implying when you say…?\n",
    "- If we do this, what is likely to happen as a result?\n",
    "- Have you considered the implications of this reasoning?\n",
    "\n",
    "Viewpoints & Perspectives – probe alternative frames.\n",
    "- From what point of view are you looking at this?\n",
    "- Is there another point of view we should consider?\n",
    "- Which of these possible viewpoints makes the most sense given the situation?\n",
    "\n",
    "Instructions:\n",
    "- Do not repeat the justification.\n",
    "- Do not state whether the verdict is correct.\n",
    "- Ask probing questions that challenge the reasoning, highlight blind spots, and open space for reconsideration.\n",
    "- Ensure the five questions you generate come from different categories where possible\n",
    "\n",
    "Output format (JSONL):\n",
    "{{\n",
    "  \"claim\": {claim},\n",
    "  \"summary\": {summary},\n",
    "  \"questions\": [\n",
    "    \"...4 questions...\"\n",
    "  ]\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b50c629",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\temp\\finetuningSocratic\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "import tqdm as notebook_tqdm\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_groq import ChatGroq\n",
    "from typing_extensions import List\n",
    "\n",
    "load_dotenv(dotenv_path=\".env\", override=True)\n",
    "\n",
    "class MoreInfoResult(BaseModel):\n",
    "    alerts: List[str] = Field([], description=\"Any alerts or warnings about the claim\")\n",
    "    summary: str = Field(\"\", description=\"A concise summary of the claim\")\n",
    "\n",
    "class SocraticQuestionsResult(BaseModel):\n",
    "    claim: str\n",
    "    summary: str\n",
    "    questions: List[str] = Field([], description=\"Five socratic questions\")\n",
    "\n",
    "#low temperature for more factual answers, \n",
    "llmGPTOSS = ChatGroq(model_name=\"openai/gpt-oss-120b\", model_kwargs={\"tool_choice\": \"none\"}, temperature=0.1)\n",
    "#llmGPT5 = ChatOpenAI(model=\"gpt-5\", temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb2657c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "def retrieve_info(claim: str, date_published: str) -> dict:\n",
    "\n",
    "    \"\"\"Gather more information about a potentially checkable claim.\"\"\"\n",
    "\n",
    "    # Use structured output\n",
    "    structured_llm = llmGPTOSS.with_structured_output(MoreInfoResult,method=\"json_mode\")\n",
    "    #structured_llm = llmGPT5.with_structured_output(MoreInfoResult,method=\"json_mode\")\n",
    "\n",
    "    # Create a prompt\n",
    "    prompt = get_information_prompt.format(\n",
    "        claim=claim,\n",
    "        date_published=date_published\n",
    "    )\n",
    "\n",
    "    #invoke the LLM and store the output\n",
    "    result = structured_llm.invoke(prompt)\n",
    "\n",
    "    # return a Python dict instead of a Pydantic model\n",
    "    return result.model_dump()\n",
    "\n",
    "\n",
    "def retrieve_info_url(claim: str, date_published: str, url: str) -> dict:\n",
    "\n",
    "    # Load the article content\n",
    "    loader = WebBaseLoader(url)\n",
    "    docs = loader.load()\n",
    "    article_text = docs[0].page_content\n",
    "\n",
    "    # Use structured output\n",
    "    structured_llm = llmGPTOSS.with_structured_output(MoreInfoResult,method=\"json_mode\")\n",
    "    #structured_llm = llmGPT5.with_structured_output(MoreInfoResult,method=\"json_mode\")\n",
    "\n",
    "    # Create a prompt\n",
    "    prompt = get_information_prompt_url.format(\n",
    "        claim=claim,\n",
    "        date_published=date_published,\n",
    "        url=url,\n",
    "        article_text=article_text\n",
    "    )\n",
    "\n",
    "    #invoke the LLM and store the output\n",
    "    result = structured_llm.invoke(prompt)\n",
    "\n",
    "    # return a Python dict instead of a Pydantic model\n",
    "    return result.model_dump()\n",
    "\n",
    "\n",
    "def generate_socratic_questions(claim: str, summary: str, alerts: List) -> SocraticQuestionsResult:\n",
    "    \n",
    "    \"\"\"Generate 2 Socratic questions from claim + summary.\"\"\"\n",
    "\n",
    "    # Use structured output\n",
    "    structured_llm = llmGPTOSS.with_structured_output(SocraticQuestionsResult,method=\"json_mode\")\n",
    "    #structured_llm = llmGPT5.with_structured_output(SocraticQuestionsResult,method=\"json_mode\")\n",
    "\n",
    "    # Create a prompt\n",
    "    prompt = socratic_questions_prompt.format(\n",
    "        claim=claim,\n",
    "        alerts=alerts,\n",
    "        summary=summary\n",
    "    )\n",
    "\n",
    "    #invoke the LLM and return the output\n",
    "    return structured_llm.invoke(prompt)\n",
    "\n",
    "def retrieve_info_and_socratic_jsonl(claim: str, date_published: str, url: str) -> list[str]:\n",
    "    \"\"\"Runs both calls and returns a list of JSONL lines.\"\"\"\n",
    "\n",
    "    # Case 1: run the first LLM to create a summary\n",
    "    info = retrieve_info(claim, date_published)\n",
    "\n",
    "    # Case 2: run the second LLM to create a summary with more details\n",
    "    info_url = retrieve_info_url(claim, date_published, url)\n",
    "\n",
    "    # Retrieve the 4 questions for Case 1\n",
    "    socratic_1 = generate_socratic_questions(\n",
    "        claim=claim,\n",
    "        summary=info.get(\"summary\"),\n",
    "        alerts=info.get(\"alerts\")\n",
    "    )\n",
    "\n",
    "    # Retrieve the 4 questions for Case 2\n",
    "    socratic_2 = generate_socratic_questions(\n",
    "        claim=claim,\n",
    "        summary=info_url.get(\"summary\"),\n",
    "        alerts=info_url.get(\"alerts\")\n",
    "    )\n",
    "\n",
    "    lines = []\n",
    "\n",
    "    # --- Case 1 ---\n",
    "    q1_main = socratic_1.questions[:2]     # first 2 questions -> separate lines\n",
    "    q1_history = socratic_1.questions[2:]  # last 2 questions -> history list\n",
    "\n",
    "    for q in q1_main:\n",
    "        obj = {\n",
    "            \"claim\": claim,\n",
    "            \"summary\": info.get(\"summary\"),\n",
    "            \"alerts\": info.get(\"alerts\"),\n",
    "            \"url_used\": False,\n",
    "            \"question\": q,\n",
    "            \"history\": q1_history\n",
    "        }\n",
    "        lines.append(json.dumps(obj, ensure_ascii=False))\n",
    "\n",
    "    # --- Case 2 ---\n",
    "    q2_main = socratic_2.questions[:2]\n",
    "    q2_history = socratic_2.questions[2:]\n",
    "\n",
    "    for q in q2_main:\n",
    "        obj = {\n",
    "            \"claim\": claim,\n",
    "            \"summary\": info_url.get(\"summary\"),\n",
    "            \"alerts\": info_url.get(\"alerts\"),\n",
    "            \"url_used\": True,\n",
    "            \"question\": q,\n",
    "            \"history\": q2_history\n",
    "        }\n",
    "        lines.append(json.dumps(obj, ensure_ascii=False))\n",
    "\n",
    "    return lines\n",
    "\n",
    "\n",
    "def claims_to_jsonl_file(claims, dates, urls, output_path: str):\n",
    "\n",
    "    \"\"\"Generate JSONL lines (4 per claim) and write to file.\"\"\"\n",
    "    \n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for c, dp, u in zip(claims, dates, urls):\n",
    "            lines = retrieve_info_and_socratic_jsonl(c, dp, u)\n",
    "            for line in lines:\n",
    "                f.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dd7301",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# The lines will be written to a JSONL file\n",
    "output_path = Path(\"Data/socratic_questions_GPTOSS3000.jsonl\")\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Generate the lines and do the writing\n",
    "claims_to_jsonl_file(factors_sample_df[\"claim\"], factors_sample_df[\"date_published\"], factors_sample_df[\"url\"], output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9cac2d",
   "metadata": {},
   "source": [
    "## Compare data for 10 claims\n",
    "\n",
    "First the summaries foor GPT5 and GPT-OSS 120B are compared side by side. In the Next step ChatGPT was asked to analyse the excel and add a column with differences. The differences were minimal. since GPT-OSS 120B, is 25 times faster and about 10 times cheaper, this was the chosen model to generate the synthetic dataset for finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd455177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport json\\nimport pandas as pd\\n\\n# paths to your uploaded jsonl files\\npath_gptoss = \"Data/socratic_questions_GPTOSS.jsonl\"\\npath_gpt5   = \"Data/socratic_questions_GPT5.jsonl\"\\n\\ndef read_jsonl(path):\\n    rows = []\\n    with open(path, \"r\", encoding=\"utf-8\") as f:\\n        for line in f:\\n            line = line.strip()\\n            if not line:\\n                continue\\n            try:\\n                rows.append(json.loads(line))\\n            except json.JSONDecodeError:\\n                # skip bad lines (or collect them if you want)\\n                continue\\n    return rows\\n\\n# load\\nrows_oss = read_jsonl(path_gptoss)\\nrows_5   = read_jsonl(path_gpt5)\\n\\n# to DataFrames, keep only needed cols, remove dupes\\ndf_oss = pd.DataFrame(rows_oss)[[\"claim\", \"url_used\", \"summary\"]].drop_duplicates()\\ndf_5   = pd.DataFrame(rows_5)[[\"claim\", \"url_used\", \"summary\"]].drop_duplicates()\\n\\n# align by claim + url_used\\nmerged = pd.merge(\\n    df_oss, df_5,\\n    on=[\"claim\", \"url_used\"],\\n    how=\"inner\",\\n    suffixes=(\"_GPTOSS\", \"_GPT5\")\\n).drop_duplicates()\\n\\n# keep ONLY the two summary columns\\nsummaries_only = merged[[\"summary_GPTOSS\", \"summary_GPT5\"]]\\n\\n# view\\nprint(summaries_only.head())\\n\\n# save to Excel\\nexcel_path = \"Data/summaries_only_comparison.xlsx\"\\nsummaries_only.to_excel(excel_path, index=False)\\n\\nprint(\"Saved to:\", excel_path)\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# paths to your uploaded jsonl files\n",
    "path_gptoss = \"Data/socratic_questions_GPTOSS.jsonl\"\n",
    "path_gpt5   = \"Data/socratic_questions_GPT5.jsonl\"\n",
    "\n",
    "def read_jsonl(path):\n",
    "    rows = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                rows.append(json.loads(line))\n",
    "            except json.JSONDecodeError:\n",
    "                # skip bad lines (or collect them if you want)\n",
    "                continue\n",
    "    return rows\n",
    "\n",
    "# load\n",
    "rows_oss = read_jsonl(path_gptoss)\n",
    "rows_5   = read_jsonl(path_gpt5)\n",
    "\n",
    "# to DataFrames, keep only needed cols, remove dupes\n",
    "df_oss = pd.DataFrame(rows_oss)[[\"claim\", \"url_used\", \"summary\"]].drop_duplicates()\n",
    "df_5   = pd.DataFrame(rows_5)[[\"claim\", \"url_used\", \"summary\"]].drop_duplicates()\n",
    "\n",
    "# align by claim + url_used\n",
    "merged = pd.merge(\n",
    "    df_oss, df_5,\n",
    "    on=[\"claim\", \"url_used\"],\n",
    "    how=\"inner\",\n",
    "    suffixes=(\"_GPTOSS\", \"_GPT5\")\n",
    ").drop_duplicates()\n",
    "\n",
    "# keep ONLY the two summary columns\n",
    "summaries_only = merged[[\"summary_GPTOSS\", \"summary_GPT5\"]]\n",
    "\n",
    "# view\n",
    "print(summaries_only.head())\n",
    "\n",
    "# save to Excel\n",
    "excel_path = \"Data/summaries_only_comparison.xlsx\"\n",
    "summaries_only.to_excel(excel_path, index=False)\n",
    "\n",
    "print(\"Saved to:\", excel_path)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3115ec6a",
   "metadata": {},
   "source": [
    "Below a comparances for the questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a77e859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport json\\nimport pandas as pd\\n\\npath_gptoss = \"Data/socratic_questions_GPTOSS.jsonl\"\\npath_gpt5   = \"Data/socratic_questions_GPT5.jsonl\"\\n\\ndef read_jsonl(path):\\n    rows = []\\n    with open(path, \"r\", encoding=\"utf-8\") as f:\\n        for line in f:\\n            line = line.strip()\\n            if not line:\\n                continue\\n            try:\\n                rows.append(json.loads(line))\\n            except json.JSONDecodeError:\\n                continue\\n    return rows\\n\\ndef add_q_index(rows):\\n\\n    df = pd.DataFrame(rows)\\n\\n    # make sure url_used exists\\n    if \"url_used\" not in df.columns:\\n        df[\"url_used\"] = False\\n\\n    # stable order inside each group\\n    df[\"_orig_order\"] = range(len(df))\\n    df = df.sort_values([\"claim\", \"url_used\", \"_orig_order\"], kind=\"stable\")\\n    df[\"q_idx\"] = df.groupby([\"claim\", \"url_used\"]).cumcount() + 1\\n\\n    return df.drop(columns=[\"_orig_order\"])\\n\\n# load\\nrows_oss = read_jsonl(path_gptoss)\\nrows_5   = read_jsonl(path_gpt5)\\n\\n# add q_idx for alignment\\ndf_oss = add_q_index(rows_oss)\\ndf_5   = add_q_index(rows_5)\\n\\n# keep needed cols + dedupe\\ndf_oss_q = df_oss[[\"claim\", \"url_used\", \"q_idx\", \"question\"]].drop_duplicates()\\ndf_5_q   = df_5[[\"claim\", \"url_used\", \"q_idx\", \"question\"]].drop_duplicates()\\n\\n# merge side-by-side\\nmerged_q = pd.merge(\\n    df_oss_q, df_5_q,\\n    on=[\"claim\", \"url_used\", \"q_idx\"],\\n    how=\"outer\",\\n    suffixes=(\"_GPTOSS\", \"_GPT5\")\\n).sort_values([\"claim\", \"url_used\", \"q_idx\"], kind=\"stable\")\\n\\n# if you ONLY want two columns:\\nquestions_only = merged_q[[\"question_GPTOSS\", \"question_GPT5\"]]\\n\\nprint(questions_only.head(10))\\n\\n# optional: save to Excel\\nout_path = \"Data/questions_side_by_side.xlsx\"\\nquestions_only.to_excel(out_path, index=False)\\nprint(\"Saved to:\", out_path)\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "path_gptoss = \"Data/socratic_questions_GPTOSS.jsonl\"\n",
    "path_gpt5   = \"Data/socratic_questions_GPT5.jsonl\"\n",
    "\n",
    "def read_jsonl(path):\n",
    "    rows = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                rows.append(json.loads(line))\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    return rows\n",
    "\n",
    "def add_q_index(rows):\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # make sure url_used exists\n",
    "    if \"url_used\" not in df.columns:\n",
    "        df[\"url_used\"] = False\n",
    "\n",
    "    # stable order inside each group\n",
    "    df[\"_orig_order\"] = range(len(df))\n",
    "    df = df.sort_values([\"claim\", \"url_used\", \"_orig_order\"], kind=\"stable\")\n",
    "    df[\"q_idx\"] = df.groupby([\"claim\", \"url_used\"]).cumcount() + 1\n",
    "\n",
    "    return df.drop(columns=[\"_orig_order\"])\n",
    "\n",
    "# load\n",
    "rows_oss = read_jsonl(path_gptoss)\n",
    "rows_5   = read_jsonl(path_gpt5)\n",
    "\n",
    "# add q_idx for alignment\n",
    "df_oss = add_q_index(rows_oss)\n",
    "df_5   = add_q_index(rows_5)\n",
    "\n",
    "# keep needed cols + dedupe\n",
    "df_oss_q = df_oss[[\"claim\", \"url_used\", \"q_idx\", \"question\"]].drop_duplicates()\n",
    "df_5_q   = df_5[[\"claim\", \"url_used\", \"q_idx\", \"question\"]].drop_duplicates()\n",
    "\n",
    "# merge side-by-side\n",
    "merged_q = pd.merge(\n",
    "    df_oss_q, df_5_q,\n",
    "    on=[\"claim\", \"url_used\", \"q_idx\"],\n",
    "    how=\"outer\",\n",
    "    suffixes=(\"_GPTOSS\", \"_GPT5\")\n",
    ").sort_values([\"claim\", \"url_used\", \"q_idx\"], kind=\"stable\")\n",
    "\n",
    "# if you ONLY want two columns:\n",
    "questions_only = merged_q[[\"question_GPTOSS\", \"question_GPT5\"]]\n",
    "\n",
    "print(questions_only.head(10))\n",
    "\n",
    "# optional: save to Excel\n",
    "out_path = \"Data/questions_side_by_side.xlsx\"\n",
    "questions_only.to_excel(out_path, index=False)\n",
    "print(\"Saved to:\", out_path)\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
