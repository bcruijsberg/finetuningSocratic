{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cae8ab7",
   "metadata": {},
   "source": [
    "## Create a sample set to generate a dataset for fine tuning.\n",
    "\n",
    "First load the FACTors data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52b82330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original rows: 118112\n",
      "Articles with multiple claims: 12\n",
      "Rows after removing duplicates: 117981\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "factors_df = pd.read_csv(\"Data/FACTors.csv\")\n",
    "\n",
    "# Identify article_ids that occur only once\n",
    "article_counts = factors_df['article_id'].value_counts()\n",
    "duplicate_article_ids = article_counts[article_counts > 1]\n",
    "unique_article_ids = article_counts[article_counts == 1].index\n",
    "\n",
    "# Filter the DataFrame to keep only unique article_ids\n",
    "clean_factors_df = factors_df[factors_df['article_id'].isin(unique_article_ids)]\n",
    "\n",
    "# Confirm removal\n",
    "print(f\"Original rows: {len(factors_df)}\")\n",
    "print(f\"Articles with multiple claims: {len(duplicate_article_ids)}\")\n",
    "print(f\"Rows after removing duplicates: {len(clean_factors_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1b081f",
   "metadata": {},
   "source": [
    "## Build a dataset with claims and factchecked answers\n",
    "Retrieve first a sample of 1000 claims and fact checked articles, make sure to divide the verdicts equally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e077d21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a subset of the largest fact checking organisations\n",
    "factors_sub_df=clean_factors_df[clean_factors_df[\"organisation\"].isin([\"PolitiFact\", \"AFP Fact Check\", \"Snopes\", \"WebQoof\", \"FactCheck.org\"])]\n",
    "factors_sample_df= factors_sub_df.sample(n=3000, random_state=12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f0c315",
   "metadata": {},
   "source": [
    "Retrieve the full articles fromt the url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ed05a6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>date_published</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>81368</th>\n",
       "      <td>\"Arizona officials caught changing ballots, ha...</td>\n",
       "      <td>2024-11-12T00:00:00</td>\n",
       "      <td>https://www.politifact.com/factchecks/2024/nov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90708</th>\n",
       "      <td>The Yeti snow monster from Disneyland's iconic...</td>\n",
       "      <td>2020-11-30T11:45:24</td>\n",
       "      <td>https://www.snopes.com/fact-check/disney-yeti/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67021</th>\n",
       "      <td>\"I can tell you that the enhanced interrogatio...</td>\n",
       "      <td>2016-05-24T00:00:00</td>\n",
       "      <td>https://www.politifact.com/factchecks/2016/may...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10591</th>\n",
       "      <td>Nigerian election tribunal witness goes on the...</td>\n",
       "      <td>2023-07-10T11:38:00</td>\n",
       "      <td>https://factcheck.afp.com/doc.afp.com.33NE7Y8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75871</th>\n",
       "      <td>\"We essentially repealed Obamacare because we ...</td>\n",
       "      <td>2017-12-21T00:00:00</td>\n",
       "      <td>https://www.politifact.com/factchecks/2017/dec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85826</th>\n",
       "      <td>A photograph showing three, white bats depicts...</td>\n",
       "      <td>2023-05-31T17:22:58</td>\n",
       "      <td>https://www.snopes.com/fact-check/cute-white-b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4068</th>\n",
       "      <td>Elderly Chinese couple hold hands in hospital ...</td>\n",
       "      <td>2020-02-25T08:10:00</td>\n",
       "      <td>https://factcheck.afp.com/these-images-have-pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76474</th>\n",
       "      <td>Says Gov. Ralph Northam \"stated that he would ...</td>\n",
       "      <td>2019-02-20T00:00:00</td>\n",
       "      <td>https://www.politifact.com/factchecks/2019/feb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99477</th>\n",
       "      <td>A high school student named Cole Withrow was c...</td>\n",
       "      <td>2013-05-06T19:13:26</td>\n",
       "      <td>https://www.snopes.com/fact-check/cole-withrow/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106164</th>\n",
       "      <td>Old Images Used to Show Massive Fire in Uttara...</td>\n",
       "      <td>2020-05-28T06:12:57</td>\n",
       "      <td>https://www.thequint.com/news/webqoof/old-imag...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    claim  \\\n",
       "81368   \"Arizona officials caught changing ballots, ha...   \n",
       "90708   The Yeti snow monster from Disneyland's iconic...   \n",
       "67021   \"I can tell you that the enhanced interrogatio...   \n",
       "10591   Nigerian election tribunal witness goes on the...   \n",
       "75871   \"We essentially repealed Obamacare because we ...   \n",
       "...                                                   ...   \n",
       "85826   A photograph showing three, white bats depicts...   \n",
       "4068    Elderly Chinese couple hold hands in hospital ...   \n",
       "76474   Says Gov. Ralph Northam \"stated that he would ...   \n",
       "99477   A high school student named Cole Withrow was c...   \n",
       "106164  Old Images Used to Show Massive Fire in Uttara...   \n",
       "\n",
       "             date_published                                                url  \n",
       "81368   2024-11-12T00:00:00  https://www.politifact.com/factchecks/2024/nov...  \n",
       "90708   2020-11-30T11:45:24     https://www.snopes.com/fact-check/disney-yeti/  \n",
       "67021   2016-05-24T00:00:00  https://www.politifact.com/factchecks/2016/may...  \n",
       "10591   2023-07-10T11:38:00      https://factcheck.afp.com/doc.afp.com.33NE7Y8  \n",
       "75871   2017-12-21T00:00:00  https://www.politifact.com/factchecks/2017/dec...  \n",
       "...                     ...                                                ...  \n",
       "85826   2023-05-31T17:22:58  https://www.snopes.com/fact-check/cute-white-b...  \n",
       "4068    2020-02-25T08:10:00  https://factcheck.afp.com/these-images-have-pr...  \n",
       "76474   2019-02-20T00:00:00  https://www.politifact.com/factchecks/2019/feb...  \n",
       "99477   2013-05-06T19:13:26    https://www.snopes.com/fact-check/cole-withrow/  \n",
       "106164  2020-05-28T06:12:57  https://www.thequint.com/news/webqoof/old-imag...  \n",
       "\n",
       "[3000 rows x 3 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factors_sample_df=factors_sample_df[['claim','date_published','url']]\n",
    "factors_sample_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "da264755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>date_published</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4582</th>\n",
       "      <td>This video shows a Rohingya man got terrified ...</td>\n",
       "      <td>2024-04-17T16:44:09</td>\n",
       "      <td>https://factcheck.afp.com/footage-shows-staged...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  claim       date_published  \\\n",
       "4582  This video shows a Rohingya man got terrified ...  2024-04-17T16:44:09   \n",
       "\n",
       "                                                    url  \n",
       "4582  https://factcheck.afp.com/footage-shows-staged...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset = factors_sample_df.iloc[1823:1824]\n",
    "subset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55203dc",
   "metadata": {},
   "source": [
    "## Generate a summary and questions\n",
    "Retrieve information and create a summary as done in the original workflow of the assistant for these 1000 claims.\n",
    "\n",
    "GPT5 and GPT OSS 120GB were compared, Since GPT OSS was much faster (53 seconds versus 24,5 min) and produced good results. This was the choice.\n",
    "- https://artificialanalysis.ai/models/gpt-oss-120b/providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fc24b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_information_prompt = \"\"\"\n",
    "### Role\n",
    "You are a neutral, guiding assistant that helps students through the fact-checking process step by step. \n",
    "In this step your are tasked with extracting detailed information about a claim to determine its checkability.\n",
    "\n",
    "### Claim\n",
    "{claim}\n",
    "\n",
    "The claim has already been fact-checked and the outcome was published on this date:\n",
    "### Date published\n",
    "{date_published}\n",
    "\n",
    "### Steps\n",
    "1. Identify the subject.\n",
    "2. Determine if the claim is *quantitative*. \n",
    "3. Assess precision: \"precise\", \"vague\", or \"absolute (100%)\". \n",
    "4. Identify what the claim is *based on* (e.g., \"survey …\", \"official statistics\"). \n",
    "5. Identify the geography and time period mentioned in the claim, if provided. You may assume that the date_published occurs shortly after the claim was made.\n",
    "6. Identify *alerts/warnings*: unclear subject, qualitative claim, vague quantitative claim, geography missing, time period missing, methodological details absent. \n",
    "Don't mention an alert when the information is present.\n",
    "7. Summarize concisely* what is currently known about the claim.\n",
    "   - Include: the information found in the first 5 steps such as subject, type (quantitative/qualitative), precision, basis, and uncertainties.\n",
    "   - Mention any active alerts or missing information.\n",
    "\n",
    "Keep your tone neutral and analytical.\n",
    "\n",
    "### Output Format\n",
    "Return a single JSON object with exactly these fields:\n",
    "\n",
    "- \"alerts\": array of strings. Each alert as a short string; use [] if none.\n",
    "- \"summary\": string. A concise summary of the claim and its checkability status.\n",
    "\n",
    "The response must be valid JSON and contain **only** this JSON object, with no extra text before or after it.\n",
    "\n",
    "### Examples\n",
    "Example A (qualitative):\n",
    "{{\n",
    "  \"alerts\": [\"qualitative claim\", \"methodological details absent\", \"geography present\", \"time period present\"],\n",
    "  \"summary\": \"A qualitative claim about a specific legal event; methodology implied but not fully detailed.\"\n",
    "}}\n",
    "\n",
    "Example B (quantitative but vague):\n",
    "{{\n",
    "  \"alerts\": [\"vague quantitative claim\", \"time period missing\", \"source/methodology missing\", \"geography: EU (present)\"],\n",
    "  \"summary\": \"A quantitative claim lacking precision and methodological details; several key elements are missing for checkability.\"\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9ac2a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_information_prompt_url = \"\"\"\n",
    "### Role\n",
    "You are a neutral, guiding assistant that helps students through the fact-checking process step by step. \n",
    "In this step your are tasked with extracting detailed information about a claim to determine its checkability.\n",
    "\n",
    "### Claim\n",
    "{claim}\n",
    "\n",
    "The claim has already been fact-checked, below the article and date published:\n",
    "### Fact-checked article\n",
    "{date_published}\n",
    "\n",
    "<Article>\n",
    "{article_text}\n",
    "</Article>\n",
    "\n",
    "### Steps\n",
    "1. Identify the subject.\n",
    "2. Determine if the claim is *quantitative*. \n",
    "3. Assess precision: \"precise\", \"vague\", or \"absolute (100%)\". \n",
    "4. Identify what the claim is *based on* (e.g., \"survey …\", \"official statistics\"). \n",
    "5. Identify the geography and time period mentioned in the claim, if provided. You may assume that the date_published occurs shortly after the claim was made.\n",
    "6. Identify *alerts/warnings*: unclear subject, qualitative claim, vague quantitative claim, geography missing, time period missing, methodological details absent. \n",
    "Don't mention an alert when the information is present.\n",
    "7. Summarize concisely* what is currently known about the claim, take into account the information in the *Article*.\n",
    "   - Include: the information found in the first 5 steps such as subject, type (quantitative/qualitative), precision, basis, and uncertainties.\n",
    "   - Check the content of the *Article* for missing information, but don't mention a *verdict* (e.g. True or False) in the summary.\n",
    "   - Mention any active alerts or missing information, that could not be found in *Article*.\n",
    "\n",
    "Keep your tone neutral and analytical.\n",
    "\n",
    "### Output Format\n",
    "Return a single JSON object with exactly these fields:\n",
    "\n",
    "- \"alerts\": array of strings. Each alert as a short string; use [] if none.\n",
    "- \"summary\": string. A concise summary of the claim and its checkability status.\n",
    "\n",
    "The response must be valid JSON and contain **only** this JSON object, with no extra text before or after it.\n",
    "\n",
    "### Examples\n",
    "Example A (qualitative):\n",
    "{{\n",
    "  \"alerts\": [\"qualitative claim\", \"methodological details absent\", \"geography present\", \"time period present\"],\n",
    "  \"summary\": \"A qualitative claim about a specific legal event; methodology implied but not fully detailed.\"\n",
    "}}\n",
    "\n",
    "Example B (quantitative but vague):\n",
    "{{\n",
    "  \"alerts\": [\"vague quantitative claim\", \"time period missing\", \"source/methodology missing\", \"geography: EU (present)\"],\n",
    "  \"summary\": \"A quantitative claim lacking precision and methodological details; several key elements are missing for checkability.\"\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e9884b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "socratic_questions_prompt = \"\"\"You are given: a claim we want to fact-check, a summary of what is currently known about the claim, \n",
    "and a set of alerts highlighting omissions or missing elements that may obstruct the fact-checking process.\n",
    "Your task is to generate 4 reflective Socratic questions that engage journalism students in critical thinking about how they interact with an AI assistant.\n",
    "Your questions should challenge the student’s reasoning, surface blind spots, and encourage deeper reflection, rather than accepting the AI’s output at face value.\n",
    "\n",
    "The questions should be reflective, probing, and open-ended—never instructional or leading.\n",
    "\n",
    "### Context\n",
    "Claim: {claim}\n",
    "Known info: {summary}\n",
    "\n",
    "<Alerts>\n",
    "{alerts}\n",
    "</Alerts>\n",
    "\n",
    "### Guidance\n",
    "The AI must pay special attention to the following dimensions (with examples):\n",
    "\n",
    "Check-worthiness\n",
    "- Why might this claim be important (or not) to fact-check?\n",
    "- Who could be affected if this claim is true, false, or misinterpreted?\n",
    "- Is this claim circulating widely enough to merit attention?\n",
    "\n",
    "Amplification Risk\n",
    "- Could engaging with this claim inadvertently spread or legitimize it?\n",
    "- How might the language used by AI contribute to amplifying the claim?\n",
    "- What safeguards should be considered to avoid reinforcing misinformation?\n",
    "\n",
    "Factuality\n",
    "- If the claim is not quantitative, how could you verify the information?\n",
    "- Could missing or weak evidence be questioned?\n",
    "\n",
    "Objectivity\n",
    "- Are we interpreting the claim or the AI’s summary through a subjective lens?\n",
    "- What perspective might be implicitly privileged?\n",
    "\n",
    "Fairness\n",
    "- Does the AI summary fairly represent the claim and all stakeholders?\n",
    "- Whose viewpoint is missing?\n",
    "\n",
    "Transparency\n",
    "- Are the AI’s reasoning steps visible and understandable?\n",
    "- What information would make the logic clearer?\n",
    "\n",
    "Hallucinations\n",
    "- Which parts of the AI’s summary are verifiable versus inferred or invented?\n",
    "\n",
    "Strategies & Alternatives\n",
    "- What alternative verification strategies could be used?\n",
    "- Is the AI narrowing the approach prematurely?\n",
    "\n",
    "Use these Socratic categories as inspiration:\n",
    "- Purpose – probe intent\n",
    "- Questions – probe whether the right questions are asked\n",
    "- Information – probe evidence and missing data\n",
    "- Inferences & Conclusions – probe reasoning\n",
    "- Concepts – probe definitions and frames\n",
    "- Assumptions – probe what is taken for granted\n",
    "- Implications – probe consequences\n",
    "- Viewpoints – probe alternative perspectives\n",
    "\n",
    "Output format (JSONL):\n",
    "{{\n",
    "  \"claim\": {claim},\n",
    "  \"summary\": {summary},\n",
    "  \"questions\": [\n",
    "    \"...4 questions...\"\n",
    "  ]\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b50c629",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\temp\\finetuningSocratic\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "import tqdm as notebook_tqdm\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_groq import ChatGroq\n",
    "from typing_extensions import List\n",
    "\n",
    "load_dotenv(dotenv_path=\".env\", override=True)\n",
    "\n",
    "class MoreInfoResult(BaseModel):\n",
    "    alerts: List[str] = Field([], description=\"Any alerts or warnings about the claim\")\n",
    "    summary: str = Field(\"\", description=\"A concise summary of the claim\")\n",
    "\n",
    "class SocraticQuestionsResult(BaseModel):\n",
    "    claim: str\n",
    "    summary: str\n",
    "    questions: List[str] = Field([], description=\"Five socratic questions\")\n",
    "\n",
    "#low temperature for more factual answers, \n",
    "llmGPTOSS = ChatGroq(model_name=\"openai/gpt-oss-120b\", model_kwargs={\"tool_choice\": \"none\"}, temperature=0.1)\n",
    "#llmGPT5 = ChatOpenAI(model=\"gpt-5\", temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb2657c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "def retrieve_info(claim: str, date_published: str) -> dict:\n",
    "\n",
    "    \"\"\"Gather more information about a potentially checkable claim.\"\"\"\n",
    "\n",
    "    # Use structured output\n",
    "    structured_llm = llmGPTOSS.with_structured_output(MoreInfoResult,method=\"json_mode\")\n",
    "    #structured_llm = llmGPT5.with_structured_output(MoreInfoResult,method=\"json_mode\")\n",
    "\n",
    "    # Create a prompt\n",
    "    prompt = get_information_prompt.format(\n",
    "        claim=claim,\n",
    "        date_published=date_published\n",
    "    )\n",
    "\n",
    "    #invoke the LLM and store the output\n",
    "    result = structured_llm.invoke(prompt)\n",
    "\n",
    "    # return a Python dict instead of a Pydantic model\n",
    "    return result.model_dump()\n",
    "\n",
    "\n",
    "def retrieve_info_url(claim: str, date_published: str, url: str) -> dict:\n",
    "\n",
    "    # Load the article content\n",
    "    loader = WebBaseLoader(url)\n",
    "    docs = loader.load()\n",
    "    article_text = docs[0].page_content\n",
    "\n",
    "    # Use structured output\n",
    "    structured_llm = llmGPTOSS.with_structured_output(MoreInfoResult,method=\"json_mode\")\n",
    "    #structured_llm = llmGPT5.with_structured_output(MoreInfoResult,method=\"json_mode\")\n",
    "\n",
    "    # Create a prompt\n",
    "    prompt = get_information_prompt_url.format(\n",
    "        claim=claim,\n",
    "        date_published=date_published,\n",
    "        url=url,\n",
    "        article_text=article_text\n",
    "    )\n",
    "\n",
    "    #invoke the LLM and store the output\n",
    "    result = structured_llm.invoke(prompt)\n",
    "\n",
    "    # return a Python dict instead of a Pydantic model\n",
    "    return result.model_dump()\n",
    "\n",
    "\n",
    "def generate_socratic_questions(claim: str, summary: str, alerts: List) -> SocraticQuestionsResult:\n",
    "    \n",
    "    \"\"\"Generate 2 Socratic questions from claim + summary.\"\"\"\n",
    "\n",
    "    # Use structured output\n",
    "    structured_llm = llmGPTOSS.with_structured_output(SocraticQuestionsResult,method=\"json_mode\")\n",
    "    #structured_llm = llmGPT5.with_structured_output(SocraticQuestionsResult,method=\"json_mode\")\n",
    "\n",
    "    # Create a prompt\n",
    "    prompt = socratic_questions_prompt.format(\n",
    "        claim=claim,\n",
    "        alerts=alerts,\n",
    "        summary=summary\n",
    "    )\n",
    "\n",
    "    #invoke the LLM and return the output\n",
    "    return structured_llm.invoke(prompt)\n",
    "\n",
    "def retrieve_info_and_socratic_jsonl(claim: str, date_published: str, url: str) -> list[str]:\n",
    "    \"\"\"Runs both calls and returns a list of JSONL lines.\"\"\"\n",
    "\n",
    "    # Case 1: run the first LLM to create a summary\n",
    "    info = retrieve_info(claim, date_published)\n",
    "\n",
    "    # Case 2: run the second LLM to create a summary with more details\n",
    "    info_url = retrieve_info_url(claim, date_published, url)\n",
    "\n",
    "    # Retrieve the 4 questions for Case 1\n",
    "    socratic_1 = generate_socratic_questions(\n",
    "        claim=claim,\n",
    "        summary=info.get(\"summary\"),\n",
    "        alerts=info.get(\"alerts\")\n",
    "    )\n",
    "\n",
    "    # Retrieve the 4 questions for Case 2\n",
    "    socratic_2 = generate_socratic_questions(\n",
    "        claim=claim,\n",
    "        summary=info_url.get(\"summary\"),\n",
    "        alerts=info_url.get(\"alerts\")\n",
    "    )\n",
    "\n",
    "    lines = []\n",
    "\n",
    "    # --- Case 1 ---\n",
    "    q1_main = socratic_1.questions[:2]     # first 2 questions -> separate lines\n",
    "    q1_history = socratic_1.questions[2:]  # last 2 questions -> history list\n",
    "\n",
    "    for q in q1_main:\n",
    "        obj = {\n",
    "            \"claim\": claim,\n",
    "            \"summary\": info.get(\"summary\"),\n",
    "            \"alerts\": info.get(\"alerts\"),\n",
    "            \"url_used\": False,\n",
    "            \"question\": q,\n",
    "            \"history\": q1_history\n",
    "        }\n",
    "        lines.append(json.dumps(obj, ensure_ascii=False))\n",
    "\n",
    "    # --- Case 2 ---\n",
    "    q2_main = socratic_2.questions[:2]\n",
    "    q2_history = socratic_2.questions[2:]\n",
    "\n",
    "    for q in q2_main:\n",
    "        obj = {\n",
    "            \"claim\": claim,\n",
    "            \"summary\": info_url.get(\"summary\"),\n",
    "            \"alerts\": info_url.get(\"alerts\"),\n",
    "            \"url_used\": True,\n",
    "            \"question\": q,\n",
    "            \"history\": q2_history\n",
    "        }\n",
    "        lines.append(json.dumps(obj, ensure_ascii=False))\n",
    "\n",
    "    return lines\n",
    "\n",
    "\n",
    "def claims_to_jsonl_file(claims, dates, urls, output_path: str):\n",
    "\n",
    "    \"\"\"Generate JSONL lines (4 per claim) and write to file.\"\"\"\n",
    "    \n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for c, dp, u in zip(claims, dates, urls):\n",
    "            lines = retrieve_info_and_socratic_jsonl(c, dp, u)\n",
    "            for line in lines:\n",
    "                f.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3dd7301",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# The lines will be written to a JSONL file\n",
    "output_path = Path(\"Data/socratic_questions_GPTOSS3000-3.jsonl\")\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Generate the lines and do the writing\n",
    "#claims_to_jsonl_file(factors_sample_df[\"claim\"], factors_sample_df[\"date_published\"], factors_sample_df[\"url\"], output_path)\n",
    "claims_to_jsonl_file(subset[\"claim\"], subset[\"date_published\"], subset[\"url\"], output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9cac2d",
   "metadata": {},
   "source": [
    "## Compare data for 10 claims\n",
    "\n",
    "First the summaries foor GPT5 and GPT-OSS 120B are compared side by side. In the Next step ChatGPT was asked to analyse the excel and add a column with differences. The differences were minimal. since GPT-OSS 120B, is 25 times faster and about 10 times cheaper, this was the chosen model to generate the synthetic dataset for finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd455177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               claim  \\\n",
      "0  \"Arizona officials caught changing ballots, ha...   \n",
      "1  \"Arizona officials caught changing ballots, ha...   \n",
      "2  The Yeti snow monster from Disneyland's iconic...   \n",
      "3  The Yeti snow monster from Disneyland's iconic...   \n",
      "4  \"I can tell you that the enhanced interrogatio...   \n",
      "\n",
      "                                      summary_GPTOSS  \\\n",
      "0  The claim states that Arizona officials were c...   \n",
      "1  The claim asserts that Arizona election offici...   \n",
      "2  The claim states that the Yeti snow monster fr...   \n",
      "3  The claim states that the Yeti snow monster fr...   \n",
      "4  The claim states that waterboarding, an enhanc...   \n",
      "\n",
      "                                        summary_GPT5  \n",
      "0  The claim asserts, in absolute terms, that unn...  \n",
      "1  Subject: Arizona election officials allegedly ...  \n",
      "2  The claim asserts that the Yeti/Abominable Sno...  \n",
      "3  The claim states that the Matterhorn ride’s Ye...  \n",
      "4  Subject: effectiveness of enhanced interrogati...  \n",
      "Saved to: Data/summaries_side_by_side.xlsx\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# paths to your uploaded jsonl files\n",
    "path_gptoss = \"Data/socratic_questions_GPTOSS.jsonl\"\n",
    "path_gpt5   = \"Data/socratic_questions_GPT5.jsonl\"\n",
    "\n",
    "def read_jsonl(path):\n",
    "    rows = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                rows.append(json.loads(line))\n",
    "            except json.JSONDecodeError:\n",
    "                # skip bad lines (or collect them if you want)\n",
    "                continue\n",
    "    return rows\n",
    "\n",
    "# load\n",
    "rows_oss = read_jsonl(path_gptoss)\n",
    "rows_5   = read_jsonl(path_gpt5)\n",
    "\n",
    "# to DataFrames, keep only needed cols, remove dupes\n",
    "df_oss = pd.DataFrame(rows_oss)[[\"claim\", \"url_used\", \"summary\"]].drop_duplicates()\n",
    "df_5   = pd.DataFrame(rows_5)[[\"claim\", \"url_used\", \"summary\"]].drop_duplicates()\n",
    "\n",
    "# align by claim + url_used\n",
    "merged = pd.merge(\n",
    "    df_oss, df_5,\n",
    "    on=[\"claim\", \"url_used\"],\n",
    "    how=\"inner\",\n",
    "    suffixes=(\"_GPTOSS\", \"_GPT5\")\n",
    ").drop_duplicates()\n",
    "\n",
    "# filter  on the claim and both summaries\n",
    "summaries_only = merged[[\"claim\",\"summary_GPTOSS\", \"summary_GPT5\"]]\n",
    "\n",
    "# view\n",
    "print(summaries_only.head())\n",
    "\n",
    "# save to Excel\n",
    "excel_path = \"Data/summaries_side_by_side.xlsx\"\n",
    "summaries_only.to_excel(excel_path, index=False)\n",
    "\n",
    "print(\"Saved to:\", excel_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3115ec6a",
   "metadata": {},
   "source": [
    "Below a comparances for the questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a77e859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     question_GPTOSS  \\\n",
      "0  What is the underlying purpose of highlighting...   \n",
      "1  What specific data sources or demographic stud...   \n",
      "2  Which assumptions are being made about who qua...   \n",
      "3  If the claim were accurate, what are the pract...   \n",
      "4  From whose perspective is this eligibility bei...   \n",
      "5  What specific data or methodology would be nee...   \n",
      "6  Why does the summary rely on a range of 500 00...   \n",
      "7  How might the framing of the claim—emphasizing...   \n",
      "8  In what ways could the lack of methodological ...   \n",
      "9  If the actual number of eligible individuals i...   \n",
      "\n",
      "                                       question_GPT5  \n",
      "0  What exactly is the fact-checkable question we...  \n",
      "1  Which specific, verifiable data sources (e.g.,...  \n",
      "2  How are you operationalizing the concepts of '...  \n",
      "3  What assumptions are you making about the clai...  \n",
      "4  From whose perspective might this statement or...  \n",
      "5  What exactly are we trying to establish: the n...  \n",
      "6  Which specific MPI report and USCIS metric (ap...  \n",
      "7  When inferring “still eligible” by subtracting...  \n",
      "8  How are you defining “particularly” for AAPI—h...  \n",
      "9  Why highlight the phrase “almost half a millio...  \n",
      "Saved to: Data/questions_side_by_side.xlsx\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "path_gptoss = \"Data/socratic_questions_GPTOSS.jsonl\"\n",
    "path_gpt5   = \"Data/socratic_questions_GPT5.jsonl\"\n",
    "\n",
    "def read_jsonl(path):\n",
    "    rows = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                rows.append(json.loads(line))\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    return rows\n",
    "\n",
    "def add_q_index(rows):\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # make sure url_used exists\n",
    "    if \"url_used\" not in df.columns:\n",
    "        df[\"url_used\"] = False\n",
    "\n",
    "    # stable order inside each group\n",
    "    df[\"_orig_order\"] = range(len(df))\n",
    "    df = df.sort_values([\"claim\", \"url_used\", \"_orig_order\"], kind=\"stable\")\n",
    "    df[\"q_idx\"] = df.groupby([\"claim\", \"url_used\"]).cumcount() + 1\n",
    "\n",
    "    return df.drop(columns=[\"_orig_order\"])\n",
    "\n",
    "# load\n",
    "rows_oss = read_jsonl(path_gptoss)\n",
    "rows_5   = read_jsonl(path_gpt5)\n",
    "\n",
    "# add q_idx for alignment\n",
    "df_oss = add_q_index(rows_oss)\n",
    "df_5   = add_q_index(rows_5)\n",
    "\n",
    "# keep needed cols + dedupe\n",
    "df_oss_q = df_oss[[\"claim\", \"url_used\", \"q_idx\", \"question\"]].drop_duplicates()\n",
    "df_5_q   = df_5[[\"claim\", \"url_used\", \"q_idx\", \"question\"]].drop_duplicates()\n",
    "\n",
    "# merge side-by-side\n",
    "merged_q = pd.merge(\n",
    "    df_oss_q, df_5_q,\n",
    "    on=[\"claim\", \"url_used\", \"q_idx\"],\n",
    "    how=\"outer\",\n",
    "    suffixes=(\"_GPTOSS\", \"_GPT5\")\n",
    ").sort_values([\"claim\", \"url_used\", \"q_idx\"], kind=\"stable\")\n",
    "\n",
    "# if you ONLY want two columns:\n",
    "questions_only = merged_q[[\"question_GPTOSS\", \"question_GPT5\"]]\n",
    "\n",
    "print(questions_only.head(10))\n",
    "\n",
    "# optional: save to Excel\n",
    "out_path = \"Data/questions_side_by_side.xlsx\"\n",
    "questions_only.to_excel(out_path, index=False)\n",
    "print(\"Saved to:\", out_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
