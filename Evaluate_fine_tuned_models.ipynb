{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da29b916",
   "metadata": {},
   "source": [
    "## Evaluation of the fine-tuned models\n",
    "\n",
    "1. create a dataset of 100 claims and summaries, excluding the onces used for training\n",
    "2. run the prompt with both models, and store the results and latency in a dataset\n",
    "3. compare the output and calculate the average latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1b509287",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# retrieve all the claims in the jsonl file used for training and store in a list\n",
    "excluded_claims = set()\n",
    "\n",
    "with open(\"Data/socratic_questions_GPTOSS3000.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        obj = json.loads(line)\n",
    "        excluded_claims.add(obj[\"claim\"])\n",
    "\n",
    "excluded_claims = list(excluded_claims)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bfb544",
   "metadata": {},
   "source": [
    "Create a filtered set without duplicates and without the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "28edc9cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original rows: 118112\n",
      "Rows after excluding specific claims: 115112\n",
      "Rows after removing duplicates: 114981\n",
      "                                                    claim       date_published\n",
      "90392   In January 2021, House Speaker Nancy Pelosi an...  2021-01-05T13:16:13\n",
      "76334   \"Pipe bomber suspect pictured last year with I...  2018-11-01T00:00:00\n",
      "61393   This video shows a cartel member in Mexico car...  2023-06-15T00:00:00\n",
      "2794    Family received $1,400 per week in government ...  2019-05-13T11:02:00\n",
      "99252   The Obama administration has ordered $1 billio...  2014-01-25T12:00:00\n",
      "...                                                   ...                  ...\n",
      "38213   Florida Gov. Charlie Crist's position on the n...  2010-07-29T14:14:27\n",
      "89479   U.S. President Joe Biden's son Hunter is guest...  2021-04-28T11:31:00\n",
      "88197   Denzel Washington said: \"You'll never be criti...  2022-04-01T11:00:30\n",
      "91521   A bottle of hand sanitizer will spontaneously ...  2020-05-22T16:46:29\n",
      "101692                                  The daughter of U  2003-03-29T12:00:00\n",
      "\n",
      "[500 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "factors_df = pd.read_csv(\"Data/FACTors.csv\")\n",
    "\n",
    "# Exclude the claims used for training\n",
    "filtered_df = factors_df[~factors_df['claim'].isin(excluded_claims)]\n",
    "\n",
    "# Identify article_ids that occur only once (after exclusion)\n",
    "article_counts = filtered_df['article_id'].value_counts()\n",
    "duplicate_article_ids = article_counts[article_counts > 1]\n",
    "unique_article_ids = article_counts[article_counts == 1].index\n",
    "\n",
    "# Keep only unique article_ids\n",
    "clean_factors_df = filtered_df[filtered_df['article_id'].isin(unique_article_ids)]\n",
    "\n",
    "# Take a subset of the largest fact checking organisations\n",
    "factors_sub_df=clean_factors_df[clean_factors_df[\"organisation\"].isin([\"PolitiFact\", \"AFP Fact Check\", \"Snopes\", \"WebQoof\", \"FactCheck.org\"])]\n",
    "factors_sample_df= factors_sub_df.sample(n=500, random_state=12)\n",
    "\n",
    "# Confirm removal\n",
    "print(f\"Original rows: {len(factors_df)}\")\n",
    "print(f\"Rows after excluding specific claims: {len(filtered_df)}\")\n",
    "print(f\"Rows after removing duplicates: {len(clean_factors_df)}\")\n",
    "print(factors_sample_df[[\"claim\",\"date_published\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cc3893",
   "metadata": {},
   "source": [
    "Now we will process the data in 3 steps:\n",
    "\n",
    "1. generate a summary and alerts from the claim, using GPT OSS 120B\n",
    "2. generate a critical question using the finetuned Llama and retrieve latency\n",
    "2. generate a critical question using the finetuned Mistral and retrieve latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e4844195",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_groq import ChatGroq\n",
    "from typing_extensions import List\n",
    "\n",
    "load_dotenv(dotenv_path=\".env\", override=True)\n",
    "\n",
    "class MoreInfoResult(BaseModel):\n",
    "    alerts: List[str] = Field([], description=\"Any alerts or warnings about the claim\")\n",
    "    summary: str = Field(\"\", description=\"A concise summary of the claim\")\n",
    "\n",
    "# lower temperature for more factual answers, \n",
    "llmGPTOSS = ChatGroq(model_name=\"openai/gpt-oss-120b\", model_kwargs={\"tool_choice\": \"none\"}, temperature=0.1)\n",
    "\n",
    "# higher temperature for more creativity in questions\n",
    "llmMistral = ChatOllama(model=\"mistral7b-q4km:latest\", temperature=0.5, base_url=\"http://localhost:11434\")\n",
    "llmLlama = ChatOllama(model=\"llama3_1-8b-q4km:latest\", temperature=0.5, base_url=\"http://localhost:11434\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9e8ad1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_information_prompt = \"\"\"\n",
    "### Role\n",
    "You are a neutral, guiding assistant that helps students through the fact-checking process step by step. \n",
    "In this step your are tasked with extracting detailed information about a claim to determine its checkability.\n",
    "\n",
    "### Claim\n",
    "{claim}\n",
    "\n",
    "The claim has already been fact-checked and the outcome was published on this date:\n",
    "### Date published\n",
    "{date_published}\n",
    "\n",
    "### Steps\n",
    "1. Identify the subject.\n",
    "2. Determine if the claim is *quantitative*. \n",
    "3. Assess precision: \"precise\", \"vague\", or \"absolute (100%)\". \n",
    "4. Identify what the claim is *based on* (e.g., \"survey â€¦\", \"official statistics\"). \n",
    "5. Identify the geography and time period mentioned in the claim, if provided. You may assume that the date_published occurs shortly after the claim was made.\n",
    "6. Identify *alerts/warnings*: unclear subject, qualitative claim, vague quantitative claim, geography missing, time period missing, methodological details absent. \n",
    "Don't mention an alert when the information is present.\n",
    "7. Summarize concisely* what is currently known about the claim.\n",
    "   - Include: the information found in the first 5 steps such as subject, type (quantitative/qualitative), precision, basis, and uncertainties.\n",
    "   - Mention any active alerts or missing information.\n",
    "\n",
    "Keep your tone neutral and analytical.\n",
    "\n",
    "### Output Format\n",
    "Return a single JSON object with exactly these fields:\n",
    "\n",
    "- \"alerts\": array of strings. Each alert as a short string; use [] if none.\n",
    "- \"summary\": string. A concise summary of the claim and its checkability status.\n",
    "\n",
    "The response must be valid JSON and contain **only** this JSON object, with no extra text before or after it.\n",
    "\n",
    "### Examples\n",
    "Example A (qualitative):\n",
    "{{\n",
    "  \"alerts\": [\"qualitative claim\", \"methodological details absent\", \"geography present\", \"time period present\"],\n",
    "  \"summary\": \"A qualitative claim about a specific legal event; methodology implied but not fully detailed.\"\n",
    "}}\n",
    "\n",
    "Example B (quantitative but vague):\n",
    "{{\n",
    "  \"alerts\": [\"vague quantitative claim\", \"time period missing\", \"source/methodology missing\", \"geography: EU (present)\"],\n",
    "  \"summary\": \"A quantitative claim lacking precision and methodological details; several key elements are missing for checkability.\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "get_socratic_question = \"\"\"\n",
    "### Role\n",
    "Pedagogical Facilitator and Socratic Coach.\n",
    "\n",
    "### Objective\n",
    "Your goal is to be a \"thought partner.\" Instead of pointing out errors, you ask one question that leads the student to discover gaps in the claim's logic or evidence themselves.\n",
    "\n",
    "### Inputs\n",
    "- {claim}\n",
    "- {summary}\n",
    "\n",
    "- Gaps in claim:\n",
    "{alerts}\n",
    "\n",
    "### Output rules (IMPORTANT)\n",
    "- Output exactly ONE Socratic question.\n",
    "- Output ONLY the question text.\n",
    "- Do NOT include explanations, prefixes, labels, or markdown.\n",
    "- The output must be a single string ending with a question mark (?).\n",
    "\n",
    "### Question:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ee5ab0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def retrieve_info(claim: str, date_published: str) -> dict:\n",
    "\n",
    "    \"\"\"Generate a summary and alerts from a claim \"\"\"\n",
    "\n",
    "    # Use structured output\n",
    "    structured_llm = llmGPTOSS.with_structured_output(MoreInfoResult,method=\"json_mode\")\n",
    "\n",
    "    # Create a prompt\n",
    "    prompt = get_information_prompt.format(\n",
    "        claim=claim,\n",
    "        date_published=date_published\n",
    "    )\n",
    "\n",
    "    #invoke the LLM and store the output\n",
    "    result = structured_llm.invoke(prompt)\n",
    "\n",
    "    # return a Python dict instead of a Pydantic model\n",
    "    return result.model_dump()\n",
    "\n",
    "\n",
    "def critical_question(llm, claim: str, summary: str, alerts: List):\n",
    "\n",
    "    \"\"\" Ask a socratic question to make the user think about the consequences of a fact checking a claim \"\"\"\n",
    "\n",
    "    # retrieve alerts and format to string for the prompt\n",
    "    alerts_str= \"\\n\".join(f\"- {a}\" for a in alerts)\n",
    "\n",
    "    # Create a prompt\n",
    "    prompt  =  get_socratic_question.format(\n",
    "        alerts=alerts_str,\n",
    "        claim=claim,\n",
    "        summary=summary,\n",
    "    )\n",
    "\n",
    "    #invoke the LLM and return the question + calculate latency\n",
    "    t0 = time.perf_counter()\n",
    "    result = llm.invoke([HumanMessage(content=prompt)])\n",
    "    latency = (time.perf_counter() - t0) * 1000\n",
    "\n",
    "    return (result.content, latency)\n",
    "\n",
    "\n",
    "def retrieve_info_and_question(claim: str, date_published: str):\n",
    "    \"\"\"Function to run all the steps\"\"\"\n",
    "\n",
    "    # run the first LLM to create a summary and alerts\n",
    "    info = retrieve_info(claim, date_published)\n",
    "\n",
    "    # retrieve fields\n",
    "    summary = info.get(\"summary\", \"\")\n",
    "    alerts = info.get(\"alerts\", [])\n",
    "\n",
    "    # Create questions with both models + latency\n",
    "    q_llama, l_llama = critical_question(llmLlama, claim, summary, alerts)\n",
    "    q_mistral, l_mistral = critical_question(llmMistral, claim, summary, alerts)\n",
    "\n",
    "    return (q_llama, l_llama,q_mistral, l_mistral)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38ec3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run loop and retrieve questions and latencies\n",
    "results = []\n",
    "claims = []\n",
    "\n",
    "for _, row in factors_sample_df.iterrows():\n",
    "    claims.append(row[\"claim\"])\n",
    "    results.append(retrieve_info_and_question(row[\"claim\"], row[\"date_published\"]))\n",
    "\n",
    "# add them to a dataset\n",
    "results_df = pd.DataFrame(\n",
    "    results,\n",
    "    columns=[\"q_llama\", \"l_llama\", \"q_mistral\", \"l_mistral\"]\n",
    ")\n",
    "\n",
    "results_df.insert(0, \"claim\", claims)\n",
    "\n",
    "results_df.to_excel(\"Data/eval_question_latency.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69bcc12",
   "metadata": {},
   "source": [
    "## We run some evaluations\n",
    "\n",
    "1. Trustworthiness: do they always generate a question\n",
    "2. Latency: generating speed in ms.\n",
    "3. Pattern diversity: how many diverse patterns are in the questions set.\n",
    "4. Semantic diversity: How do the questions differ in semantics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416e1528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For llama there are 49 empty rows\n",
      "For mistral there are 0 empty rows\n"
     ]
    }
   ],
   "source": [
    "# trustworthiness\n",
    "empty_llama_count = (results_df[\"q_llama\"].astype(str).str.strip() == \"\").sum()\n",
    "empty_mistral_count = (results_df[\"q_mistral\"].astype(str).str.strip() == \"\").sum()\n",
    "\n",
    "print(f\"For llama there are {empty_llama_count} empty rows\")\n",
    "print(f\"For mistral there are {empty_mistral_count} empty rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ed03bcf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latency for llama: 440 ms\n",
      "Latency for mistral: 412 ms\n"
     ]
    }
   ],
   "source": [
    "# Calculate average latency\n",
    "llama_latency=results_df[\"l_llama\"].mean()\n",
    "mistral_latency=results_df[\"l_mistral\"].mean()\n",
    "\n",
    "print(f\"Latency for llama: {llama_latency:.0f} ms\")\n",
    "print(f\"Latency for mistral: {mistral_latency:.0f} ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9629b365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For LLaMA there are 452 unique patterns and the entropy is 8.42\n",
      "For Mistral there are 499 unique patterns and the entropy is 8.96\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# number of different patterns, and diversity in patterns (entropy)\n",
    "def pattern_diversity(series):\n",
    "    counts = series.value_counts()\n",
    "    probs = counts / counts.sum()\n",
    "    entropy = -(probs * np.log2(probs)).sum()\n",
    "    return {\n",
    "        \"unique_patterns\": len(counts),\n",
    "        \"entropy\": entropy,\n",
    "    }\n",
    "\n",
    "\n",
    "div_llama = pattern_diversity(results_df[\"q_llama\"])\n",
    "div_mistral = pattern_diversity(results_df[\"q_mistral\"])\n",
    "\n",
    "print(\n",
    "    f\"For LLaMA there are {div_llama['unique_patterns']} unique patterns \"\n",
    "    f\"and the entropy is {div_llama['entropy']:.2f}\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"For Mistral there are {div_mistral['unique_patterns']} unique patterns \"\n",
    "    f\"and the entropy is {div_mistral['entropy']:.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b9375a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic diversity (LLaMA): 0.81\n",
      "Semantic diversity (Mistral): 0.74\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "import numpy as np\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# calculate semantic\n",
    "def embedding_diversity(texts):\n",
    "    texts = texts.dropna()\n",
    "    emb = model.encode(texts, normalize_embeddings=True)\n",
    "    dists = cosine_distances(emb)\n",
    "    return dists[np.triu_indices_from(dists, k=1)].mean()\n",
    "\n",
    "div_llama = embedding_diversity(results_df[\"q_llama\"])\n",
    "div_mistral = embedding_diversity(results_df[\"q_mistral\"])\n",
    "\n",
    "print(f\"Semantic diversity (LLaMA): {div_llama:.2f}\")\n",
    "print(f\"Semantic diversity (Mistral): {div_mistral:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
