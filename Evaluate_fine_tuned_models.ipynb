{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da29b916",
   "metadata": {},
   "source": [
    "## Evaluation of the fine-tuned models\n",
    "\n",
    "1. create a dataset of 100 claims and summaries, excluding the onces used for training\n",
    "2. run the prompt with both models, and store the results and latency in a dataset\n",
    "3. compare the output and calculate the average latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b509287",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# retrieve all the claims in the jsonl file used for training and store in a list\n",
    "excluded_claims = set()\n",
    "\n",
    "with open(\"Data/socratic_questions_GPTOSS3000.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        obj = json.loads(line)\n",
    "        excluded_claims.add(obj[\"claim\"])\n",
    "\n",
    "excluded_claims = list(excluded_claims)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bfb544",
   "metadata": {},
   "source": [
    "Create a filtered set without duplicates and without the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28edc9cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original rows: 118112\n",
      "Rows after excluding specific claims: 115112\n",
      "Rows after removing duplicates: 114981\n",
      "                                                    claim       date_published\n",
      "90392   In January 2021, House Speaker Nancy Pelosi an...  2021-01-05T13:16:13\n",
      "76334   \"Pipe bomber suspect pictured last year with I...  2018-11-01T00:00:00\n",
      "61393   This video shows a cartel member in Mexico car...  2023-06-15T00:00:00\n",
      "2794    Family received $1,400 per week in government ...  2019-05-13T11:02:00\n",
      "99252   The Obama administration has ordered $1 billio...  2014-01-25T12:00:00\n",
      "...                                                   ...                  ...\n",
      "38213   Florida Gov. Charlie Crist's position on the n...  2010-07-29T14:14:27\n",
      "89479   U.S. President Joe Biden's son Hunter is guest...  2021-04-28T11:31:00\n",
      "88197   Denzel Washington said: \"You'll never be criti...  2022-04-01T11:00:30\n",
      "91521   A bottle of hand sanitizer will spontaneously ...  2020-05-22T16:46:29\n",
      "101692                                  The daughter of U  2003-03-29T12:00:00\n",
      "\n",
      "[500 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "factors_df = pd.read_csv(\"Data/FACTors.csv\")\n",
    "\n",
    "# Exclude the claims used for training\n",
    "filtered_df = factors_df[~factors_df['claim'].isin(excluded_claims)]\n",
    "\n",
    "# Identify article_ids that occur only once (after exclusion)\n",
    "article_counts = filtered_df['article_id'].value_counts()\n",
    "duplicate_article_ids = article_counts[article_counts > 1]\n",
    "unique_article_ids = article_counts[article_counts == 1].index\n",
    "\n",
    "# Keep only unique article_ids\n",
    "clean_factors_df = filtered_df[filtered_df['article_id'].isin(unique_article_ids)]\n",
    "\n",
    "# Take a subset of the largest fact checking organisations\n",
    "factors_sub_df=clean_factors_df[clean_factors_df[\"organisation\"].isin([\"PolitiFact\", \"AFP Fact Check\", \"Snopes\", \"WebQoof\", \"FactCheck.org\"])]\n",
    "factors_sample_df= factors_sub_df.sample(n=500, random_state=12)\n",
    "\n",
    "# Confirm removal\n",
    "print(f\"Original rows: {len(factors_df)}\")\n",
    "print(f\"Rows after excluding specific claims: {len(filtered_df)}\")\n",
    "print(f\"Rows after removing duplicates: {len(clean_factors_df)}\")\n",
    "print(factors_sample_df[[\"claim\",\"date_published\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cc3893",
   "metadata": {},
   "source": [
    "Now we will process the data in 3 steps:\n",
    "\n",
    "1. generate a summary and alerts from the claim, using GPT OSS 120B\n",
    "2. generate a critical question using the finetuned Llama and retrieve latency\n",
    "2. generate a critical question using the finetuned Mistral and retrieve latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4844195",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\temp\\finetuningSocratic\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_groq import ChatGroq\n",
    "from typing_extensions import List\n",
    "\n",
    "load_dotenv(dotenv_path=\".env\", override=True)\n",
    "\n",
    "class MoreInfoResult(BaseModel):\n",
    "    alerts: List[str] = Field([], description=\"Any alerts or warnings about the claim\")\n",
    "    summary: str = Field(\"\", description=\"A concise summary of the claim\")\n",
    "\n",
    "# lower temperature for more factual answers, \n",
    "llmGPTOSS = ChatGroq(model_name=\"openai/gpt-oss-120b\", model_kwargs={\"tool_choice\": \"none\"}, temperature=0.1)\n",
    "\n",
    "# higher temperature for more creativity in questions\n",
    "#fine-tuned models\n",
    "llmMistral = ChatOllama(model=\"mistral7b-q4km:latest\", temperature=0.5, base_url=\"http://localhost:11434\")\n",
    "llmLlama = ChatOllama(model=\"llama3_1-8b-q4km:latest\", temperature=0.5, base_url=\"http://localhost:11434\")\n",
    "\n",
    "# original instruct models\n",
    "llmMistralInstr = ChatOllama(model=\"mistral7b-q4km:latest\", temperature=0.5, base_url=\"http://localhost:11434\")\n",
    "llmLlamaInstr = ChatOllama(model=\"llama3_1-8b-q4km:latest\", temperature=0.5, base_url=\"http://localhost:11434\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8ad1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_information_prompt = \"\"\"\n",
    "### Role\n",
    "You are a neutral, guiding assistant that helps students through the fact-checking process step by step. \n",
    "In this step your are tasked with extracting detailed information about a claim to determine its checkability.\n",
    "\n",
    "### Claim\n",
    "{claim}\n",
    "\n",
    "The claim has already been fact-checked and the outcome was published on this date:\n",
    "### Date published\n",
    "{date_published}\n",
    "\n",
    "### Steps\n",
    "1. Identify the subject.\n",
    "2. Determine if the claim is *quantitative*. \n",
    "3. Assess precision: \"precise\", \"vague\", or \"absolute (100%)\". \n",
    "4. Identify what the claim is *based on* (e.g., \"survey …\", \"official statistics\"). \n",
    "5. Identify the geography and time period mentioned in the claim, if provided. You may assume that the date_published occurs shortly after the claim was made.\n",
    "6. Identify *alerts/warnings*: unclear subject, qualitative claim, vague quantitative claim, geography missing, time period missing, methodological details absent. \n",
    "Don't mention an alert when the information is present.\n",
    "7. Summarize concisely* what is currently known about the claim.\n",
    "   - Include: the information found in the first 5 steps such as subject, type (quantitative/qualitative), precision, basis, and uncertainties.\n",
    "   - Mention any active alerts or missing information.\n",
    "\n",
    "Keep your tone neutral and analytical.\n",
    "\n",
    "### Output Format\n",
    "Return a single JSON object with exactly these fields:\n",
    "\n",
    "- \"alerts\": array of strings. Each alert as a short string; use [] if none.\n",
    "- \"summary\": string. A concise summary of the claim and its checkability status.\n",
    "\n",
    "The response must be valid JSON and contain **only** this JSON object, with no extra text before or after it.\n",
    "\n",
    "### Examples\n",
    "Example A (qualitative):\n",
    "{{\n",
    "  \"alerts\": [\"qualitative claim\", \"methodological details absent\", \"geography present\", \"time period present\"],\n",
    "  \"summary\": \"A qualitative claim about a specific legal event; methodology implied but not fully detailed.\"\n",
    "}}\n",
    "\n",
    "Example B (quantitative but vague):\n",
    "{{\n",
    "  \"alerts\": [\"vague quantitative claim\", \"time period missing\", \"source/methodology missing\", \"geography: EU (present)\"],\n",
    "  \"summary\": \"A quantitative claim lacking precision and methodological details; several key elements are missing for checkability.\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "get_socratic_question = \"\"\"\n",
    "### Role\n",
    "Pedagogical Facilitator and Socratic Coach.\n",
    "\n",
    "### Objective\n",
    "Your goal is to be a \"thought partner.\" Instead of pointing out errors, you ask one question that leads the student to discover gaps in the claim's logic or evidence themselves.\n",
    "\n",
    "### Inputs\n",
    "- {claim}\n",
    "- {summary}\n",
    "\n",
    "- Gaps in claim:\n",
    "{alerts}\n",
    "\n",
    "### Output rules (IMPORTANT)\n",
    "- Output exactly ONE Socratic question.\n",
    "- Output ONLY the question text.\n",
    "- Do NOT include explanations, prefixes, labels, or markdown.\n",
    "- The output must be a single string ending with a question mark (?).\n",
    "\n",
    "Output:\n",
    "\"\"\"\n",
    "\n",
    "socratic_questions_prompt = \"\"\"\n",
    "### Role\n",
    "You are given: a claim we want to fact-check, a summary of what is currently known about the claim, \n",
    "and a set of alerts highlighting omissions or missing elements that may obstruct the fact-checking process.\n",
    "Your task is to generate 4 reflective Socratic questions that engage journalism students in critical thinking about how they interact with an AI assistant.\n",
    "Your questions should challenge the student’s reasoning, surface blind spots, and encourage deeper reflection, rather than accepting the AI’s output at face value.\n",
    "\n",
    "The questions should be reflective, probing, and open-ended never instructional or leading.\n",
    "\n",
    "### Context\n",
    "Claim: {claim}\n",
    "Known info: {summary}\n",
    "\n",
    "<Alerts>\n",
    "{alerts}\n",
    "</Alerts>\n",
    "\n",
    "### Guidance\n",
    "The AI must pay special attention to the following dimensions (with examples):\n",
    "\n",
    "Check-worthiness\n",
    "- Why might this claim be important (or not) to fact-check?\n",
    "- Who could be affected if this claim is true, false, or misinterpreted?\n",
    "- Is this claim circulating widely enough to merit attention?\n",
    "\n",
    "Amplification Risk\n",
    "- Could engaging with this claim inadvertently spread or legitimize it?\n",
    "- How might the language used by AI contribute to amplifying the claim?\n",
    "- What safeguards should be considered to avoid reinforcing misinformation?\n",
    "\n",
    "Factuality\n",
    "- If the claim is not quantitative, how could you verify the information?\n",
    "- Could missing or weak evidence be questioned?\n",
    "\n",
    "Objectivity\n",
    "- Are we interpreting the claim or the AI’s summary through a subjective lens?\n",
    "- What perspective might be implicitly privileged?\n",
    "\n",
    "Fairness\n",
    "- Does the AI summary fairly represent the claim and all stakeholders?\n",
    "- Whose viewpoint is missing?\n",
    "\n",
    "Transparency\n",
    "- Are the AI’s reasoning steps visible and understandable?\n",
    "- What information would make the logic clearer?\n",
    "\n",
    "Hallucinations\n",
    "- Which parts of the AI’s summary are verifiable versus inferred or invented?\n",
    "\n",
    "Strategies & Alternatives\n",
    "- What alternative verification strategies could be used?\n",
    "- Is the AI narrowing the approach prematurely?\n",
    "\n",
    "Use these Socratic categories as inspiration:\n",
    "- Purpose – probe intent\n",
    "- Questions – probe whether the right questions are asked\n",
    "- Information – probe evidence and missing data\n",
    "- Inferences & Conclusions – probe reasoning\n",
    "- Concepts – probe definitions and frames\n",
    "- Assumptions – probe what is taken for granted\n",
    "- Implications – probe consequences\n",
    "- Viewpoints – probe alternative perspectives\n",
    "\n",
    "### Output rules (IMPORTANT)\n",
    "- Output exactly ONE Socratic question.\n",
    "- Output ONLY the question text.\n",
    "- Do NOT include explanations, prefixes, labels, or markdown.\n",
    "- The output must be a single string ending with a question mark (?).\n",
    "\n",
    "Output:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee5ab0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def retrieve_info(claim: str, date_published: str) -> dict:\n",
    "\n",
    "    \"\"\"Generate a summary and alerts from a claim \"\"\"\n",
    "\n",
    "    # Use structured output\n",
    "    structured_llm = llmGPTOSS.with_structured_output(MoreInfoResult,method=\"json_mode\")\n",
    "\n",
    "    # Create a prompt\n",
    "    prompt = get_information_prompt.format(\n",
    "        claim=claim,\n",
    "        date_published=date_published\n",
    "    )\n",
    "\n",
    "    #invoke the LLM and store the output\n",
    "    result = structured_llm.invoke(prompt)\n",
    "\n",
    "    # return a Python dict instead of a Pydantic model\n",
    "    return result.model_dump()\n",
    "\n",
    "\n",
    "def critical_question(llm, claim: str, summary: str, alerts: List):\n",
    "\n",
    "    \"\"\" Ask a socratic question to make the user think about the consequences of a fact checking a claim \"\"\"\n",
    "\n",
    "    # retrieve alerts and format to string for the prompt\n",
    "    alerts_str= \"\\n\".join(f\"- {a}\" for a in alerts)\n",
    "\n",
    "    # Create a prompt\n",
    "    prompt  =  get_socratic_question.format(\n",
    "        alerts=alerts_str,\n",
    "        claim=claim,\n",
    "        summary=summary,\n",
    "    )\n",
    "\n",
    "    #invoke the LLM and return the question + calculate latency\n",
    "    t0 = time.perf_counter()\n",
    "    result = llm.invoke([HumanMessage(content=prompt)])\n",
    "    latency = (time.perf_counter() - t0) * 1000\n",
    "\n",
    "    return (result.content, latency)\n",
    "\n",
    "# a different prompt is used for the original models, with examples (few-shot)\n",
    "def critical_question_original(llm, claim: str, summary: str, alerts: List):\n",
    "\n",
    "    \"\"\" Ask a socratic question to make the user think about the consequences of a fact checking a claim \"\"\"\n",
    "\n",
    "    # retrieve alerts and format to string for the prompt\n",
    "    alerts_str= \"\\n\".join(f\"- {a}\" for a in alerts)\n",
    "\n",
    "    # Create a prompt\n",
    "    prompt  =  socratic_questions_prompt.format(\n",
    "        alerts=alerts_str,\n",
    "        claim=claim,\n",
    "        summary=summary,\n",
    "    )\n",
    "\n",
    "    #invoke the LLM and return the question + calculate latency\n",
    "    t0 = time.perf_counter()\n",
    "    result = llm.invoke([HumanMessage(content=prompt)])\n",
    "    latency = (time.perf_counter() - t0) * 1000\n",
    "\n",
    "    return (result.content, latency)\n",
    "\n",
    "# This is were everything comes together\n",
    "def retrieve_info_and_question(claim: str, date_published: str):\n",
    "    \"\"\"Function to run all the steps\"\"\"\n",
    "\n",
    "    # run the first LLM to create a summary and alerts\n",
    "    info = retrieve_info(claim, date_published)\n",
    "\n",
    "    # retrieve fields\n",
    "    summary = info.get(\"summary\", \"\")\n",
    "    alerts = info.get(\"alerts\", [])\n",
    "\n",
    "    # Create questions for all models + latency\n",
    "    q_llama, l_llama = critical_question(llmLlama, claim, summary, alerts)\n",
    "    q_mistral, l_mistral = critical_question(llmMistral, claim, summary, alerts)\n",
    "    q_llama_original, l_llama_original = critical_question_original(llmLlama, claim, summary, alerts)\n",
    "    q_mistral_original, l_mistral_original = critical_question_original(llmMistral, claim, summary, alerts)\n",
    "\n",
    "    return (q_llama, l_llama, q_mistral, l_mistral, q_llama_original, l_llama_original, q_mistral_original, l_mistral_original)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e38ec3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run loop and retrieve questions and latencies\n",
    "results = []\n",
    "claims = []\n",
    "\n",
    "for _, row in factors_sample_df.iterrows():\n",
    "    claims.append(row[\"claim\"])\n",
    "    results.append(retrieve_info_and_question(row[\"claim\"], row[\"date_published\"]))\n",
    "\n",
    "# add them to a dataset\n",
    "results_df = pd.DataFrame(\n",
    "    results,\n",
    "    columns=[\"q_llama\", \"l_llama\", \"q_mistral\", \"l_mistral\", \"q_llama_o\", \"l_llama_o\", \"q_mistral_o\", \"l_mistral_o\"]\n",
    ")\n",
    "\n",
    "results_df.insert(0, \"claim\", claims)\n",
    "\n",
    "results_df.to_excel(\"Data/eval_question_latency.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69bcc12",
   "metadata": {},
   "source": [
    "## We run some evaluations\n",
    "\n",
    "1. Trustworthiness: do they always generate a question\n",
    "2. Latency: generating speed in ms.\n",
    "3. Pattern diversity: how many diverse patterns are in the questions set.\n",
    "4. Semantic diversity: How do the questions differ in semantics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "416e1528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For llama fine-tuned there are 0 empty rows\n",
      "For mistral fine-tuned there are 0 empty rows\n",
      "For llama original there are 0 empty rows\n",
      "For mistral original there are 0 empty rows\n"
     ]
    }
   ],
   "source": [
    "# trustworthiness\n",
    "empty_llama_count = (results_df[\"q_llama\"].astype(str).str.strip() == \"\").sum()\n",
    "empty_mistral_count = (results_df[\"q_mistral\"].astype(str).str.strip() == \"\").sum()\n",
    "empty_llama_o_count = (results_df[\"q_llama_o\"].astype(str).str.strip() == \"\").sum()\n",
    "empty_mistral_o_count = (results_df[\"q_mistral_o\"].astype(str).str.strip() == \"\").sum()\n",
    "\n",
    "print(f\"For llama fine-tuned there are {empty_llama_count} empty rows\")\n",
    "print(f\"For mistral fine-tuned there are {empty_mistral_count} empty rows\")\n",
    "print(f\"For llama original there are {empty_llama_o_count} empty rows\")\n",
    "print(f\"For mistral original there are {empty_mistral_o_count} empty rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed03bcf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latency for llama fine-tuned: 475 ms\n",
      "Latency for mistral fine-tuned: 418 ms\n",
      "Latency for llama original: 582 ms\n",
      "Latency for mistral original: 502 ms\n"
     ]
    }
   ],
   "source": [
    "# Calculate average latency\n",
    "llama_latency=results_df[\"l_llama\"].mean()\n",
    "mistral_latency=results_df[\"l_mistral\"].mean()\n",
    "llama_o_latency=results_df[\"l_llama_o\"].mean()\n",
    "mistral_o_latency=results_df[\"l_mistral_o\"].mean()\n",
    "\n",
    "print(f\"Latency for llama fine-tuned: {llama_latency:.0f} ms\")\n",
    "print(f\"Latency for mistral fine-tuned: {mistral_latency:.0f} ms\")\n",
    "print(f\"Latency for llama original: {llama_o_latency:.0f} ms\")\n",
    "print(f\"Latency for mistral original: {mistral_o_latency:.0f} ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9629b365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For llama fine-tuned there are 500 unique patterns and the entropy is 8.97\n",
      "For mistral fine-tuned there are 499 unique patterns and the entropy is 8.96\n",
      "For llama original there are 497 unique patterns and the entropy is 8.95\n",
      "For mistral original there are 487 unique patterns and the entropy is 8.91\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# number of different patterns, and diversity in patterns (entropy)\n",
    "def pattern_diversity(series):\n",
    "    counts = series.value_counts()\n",
    "    probs = counts / counts.sum()\n",
    "    entropy = -(probs * np.log2(probs)).sum()\n",
    "    return {\n",
    "        \"unique_patterns\": len(counts),\n",
    "        \"entropy\": entropy,\n",
    "    }\n",
    "\n",
    "\n",
    "div_llama = pattern_diversity(results_df[\"q_llama\"])\n",
    "div_mistral = pattern_diversity(results_df[\"q_mistral\"])\n",
    "div_llama_o = pattern_diversity(results_df[\"q_llama_o\"])\n",
    "div_mistral_o = pattern_diversity(results_df[\"q_mistral_o\"])\n",
    "\n",
    "print(\n",
    "    f\"For llama fine-tuned there are {div_llama['unique_patterns']} unique patterns \"\n",
    "    f\"and the entropy is {div_llama['entropy']:.2f}\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"For mistral fine-tuned there are {div_mistral['unique_patterns']} unique patterns \"\n",
    "    f\"and the entropy is {div_mistral['entropy']:.2f}\"\n",
    ")\n",
    "print(\n",
    "    f\"For llama original there are {div_llama_o['unique_patterns']} unique patterns \"\n",
    "    f\"and the entropy is {div_llama_o['entropy']:.2f}\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"For mistral original there are {div_mistral_o['unique_patterns']} unique patterns \"\n",
    "    f\"and the entropy is {div_mistral_o['entropy']:.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9375a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic diversity for llama fine-tuned: 0.78\n",
      "Semantic diversity for mistral  fine-tuned: 0.74\n",
      "Semantic diversity for llama original: 0.66\n",
      "Semantic diversity for mistral original: 0.59\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "import numpy as np\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# calculate semantic\n",
    "def embedding_diversity(texts):\n",
    "    texts = texts.dropna()\n",
    "    emb = model.encode(texts, normalize_embeddings=True)\n",
    "    dists = cosine_distances(emb)\n",
    "    return dists[np.triu_indices_from(dists, k=1)].mean()\n",
    "\n",
    "div_llama = embedding_diversity(results_df[\"q_llama\"])\n",
    "div_mistral = embedding_diversity(results_df[\"q_mistral\"])\n",
    "div_llama_o = embedding_diversity(results_df[\"q_llama_o\"])\n",
    "div_mistral_o = embedding_diversity(results_df[\"q_mistral_o\"])\n",
    "\n",
    "print(f\"Semantic diversity for llama fine-tuned: {div_llama:.2f}\")\n",
    "print(f\"Semantic diversity for mistral  fine-tuned: {div_mistral:.2f}\")\n",
    "print(f\"Semantic diversity for llama original: {div_llama_o:.2f}\")\n",
    "print(f\"Semantic diversity for mistral original: {div_mistral_o:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
