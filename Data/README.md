# Fine-Tuning Socratic Fact-Checking Models

This repository contains the code used to **fine-tune quantized large language models** for generating **Socratic-style questions** in the context of **journalistic fact-checking**.

Specifically, the project focuses on fine-tuning:

- **LLaMA 3.1 (8B, quantized)**
- **Mistral 7B (quantized)**

The fine-tuning process was implemented using the **Unsloth** library, enabling efficient training on consumer-grade hardware and cloud environments.

The resulting models were designed to support **critical reflection** during fact-checking workflows and were subsequently integrated into the **CheckMate** multi-agent assistant.

---

## ğŸ“š FACTors Dataset

Training and evaluation data were grounded in **real-world fact-checking claims** from the **FACTors project**, a large-scale dataset for studying the fact-checking ecosystem.

- FACTors GitHub repository:  
  ğŸ‘‰ https://github.com/altuncu/FACTors

Synthetic training data was generated by transforming FACTors claims into **Socratic questions**, enabling the models to learn reflective, pedagogically oriented questioning behavior.

---

## ğŸ§  Purpose of the Models

The fine-tuned models are intended to:

- Generate **critical, reflective questions** during fact-checking
- Encourage **methodological transparency**
- Support **journalism education**, rather than automate verdicts

These models were evaluated against strong instruct baselines to assess improvements in **question quality, pedagogical alignment, and latency**.

---

## ğŸ“ Repository Structure

```text
.
â”œâ”€â”€ Create_finetune_data.ipynb        # Generate synthetic training data from FACTors claims
â”‚                                    # (Socratic question generation)
â”‚
â”œâ”€â”€ Data/                             # All datasets used in training and evaluation
â”‚   â”œâ”€â”€ socratic_questions_GPTOSS3000 # Primary synthetic training dataset
â”‚   â”œâ”€â”€ socratic_questions_GPT5       # Dataset used to evaluate GPT-5 as a base model
â”‚   â”œâ”€â”€ socratic_questions_GPTOSS     # Dataset used to evaluate GPT-OSS models
â”‚   â”œâ”€â”€ summaries_side_by_side.xlsx   # Excel comparison of model-generated summaries
â”‚   â”œâ”€â”€ questions_side_by_side.xlsx  # Excel comparison of generated questions
â”‚   â””â”€â”€ eval_question_latency.xlsx   # Latency measurements for evaluated models
â”‚
â”œâ”€â”€ Evaluate_fine_tuned_models.ipynb  # Evaluation of fine-tuned models vs. instruct models
â”‚
â”œâ”€â”€ fine-tuning-llama_on_pc.ipynb     # Initial LLaMA fine-tuning experiments on Windows
â”‚                                    # (includes tokenizer-related issues)
â”‚
â”œâ”€â”€ fine-tuning-mistral.ipynb         # Final fine-tuning pipeline (run on Google Colab)
â”‚                                    # Successfully used for both LLaMA and Mistral
â”‚
â””â”€â”€ README.md                         # Project documentation
