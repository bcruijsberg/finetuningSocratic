{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96dd3f8b",
   "metadata": {},
   "source": [
    "# Finetuning an model\n",
    "\n",
    "these 4 models will be finetuned, with synthetic data based upon real fact-checked claims:\n",
    "- \"unsloth/mistral-7b-bnb-4bit\"\n",
    "- \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit \",\n",
    "- \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\"\n",
    "- \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit \"\n",
    "\n",
    "We start with Installing Unsloth, if you work on Colab use the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59232f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os, re\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    import torch; v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)\n",
    "    xformers = \"xformers==\" + (\"0.0.33.post1\" if v==\"2.9\" else \"0.0.32.post2\" if v==\"2.8\" else \"0.0.29.post3\")\n",
    "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "    !pip install --no-deps unsloth\n",
    "!pip install transformers==4.56.2\n",
    "!pip install --no-deps trl==0.22.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd77850f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.11.3: Fast Mistral patching. Transformers: 4.57.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 5060 Ti. Num GPUs = 1. Max memory: 15.928 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.9.0+cu128. CUDA: 12.0. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "model_id = \"unsloth/mistral-7b-bnb-4bit\"\n",
    "#model_id = \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit \",\n",
    "#model_id = \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\"\n",
    "#model_id = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit \"\n",
    "\n",
    "max_seq_length = 4096\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_id,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d133a80",
   "metadata": {},
   "source": [
    "Next the dataset is loaded with 12000 samples in a JSONL file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfbd60a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'claim': '\"Arizona officials caught changing ballots, have been arrested.\"',\n",
       " 'summary': 'The claim states that Arizona officials were caught changing ballots and have been arrested. It is a qualitative, absolute assertion with no cited source or methodology; the geography (Arizona) is clear, but the time frame of the alleged arrests is not provided.',\n",
       " 'alerts': ['qualitative claim',\n",
       "  'source/methodology missing',\n",
       "  'time period missing'],\n",
       " 'url_used': False,\n",
       " 'question': 'What kinds of evidence would be needed to determine whether any Arizona officials were actually arrested for altering ballots, and how might the absence of a time frame affect that search?',\n",
       " 'history': ['How might the spread of an unverified claim about election officials influence public trust, and what responsibilities do journalists have when encountering such statements?',\n",
       "  'Which perspectives or stakeholders are missing from the current description, and how could including them change the way we evaluate the claim’s credibility?']}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load the training data\n",
    "training_data = []\n",
    "with open(\"socratic_questions_GPTOSS3000.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        training_data.append(json.loads(line))\n",
    "\n",
    "ds = Dataset.from_list(training_data)\n",
    "ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280ba3a2",
   "metadata": {},
   "source": [
    "We will use the Alpaca prompt template to structure in- and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b529f9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token  # Llama/Mistral EOS like <|end_of_text|>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6868a3",
   "metadata": {},
   "source": [
    "All the context is added to the template and glued together in a string. The questions are examples of what the output should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60d9585",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 12000/12000 [00:00<00:00, 88086.48 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Generate a reflective Socratic questions to challenge the student’s reasoning, surface blind spots, and encourage deeper reflection, rather than accepting the AI’s output at face value.Differentiate the question from previous questions asked about the claim.\n",
      "\n",
      "### Input:\n",
      "[[The claim is \"Arizona officials caught changing ballots, have been arrested.\".]]\n",
      "[[This is all the information known about the claim: The claim states that Arizona officials were caught changing ballots and have been arrested. It is a qualitative, absolute assertion with no cited source or methodology; the geography (Arizona) is clear, but the time frame of the alleged arrests is not provided..]]\n",
      "[[These alerts point out omissions: qualitative claim, source/methodology missing, time period missing.]]\n",
      "[[Take into account previous questions asked: \n",
      "- How might the spread of an unverified claim about election officials influence public trust, and what responsibilities do journalists have when encountering such statements?\n",
      "- Which perspectives or stakeholders are missing from the current description, and how could including them change the way we evaluate the claim’s credibility?]]\n",
      "\n",
      "### Response:\n",
      "What kinds of evidence would be needed to determine whether any Arizona officials were actually arrested for altering ballots, and how might the absence of a time frame affect that search?</s>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def to_alpaca_text(examples):\n",
    "    texts = []\n",
    "\n",
    "    for claim, summary, alerts, history, question in zip(\n",
    "        examples[\"claim\"],\n",
    "        examples[\"summary\"],\n",
    "        examples[\"alerts\"],\n",
    "        examples.get(\"history\", [[]]*len(examples[\"claim\"])),\n",
    "        examples[\"question\"],\n",
    "    ):\n",
    "        # 1) instruction: fixed goal of the model\n",
    "        instruction = (\n",
    "            \"Generate a reflective Socratic questions to challenge the student’s reasoning, surface blind spots, and encourage deeper reflection, rather than accepting the AI’s output at face value.\"\n",
    "            \"Differentiate the question from previous questions asked about the claim.\"\n",
    "        )\n",
    "\n",
    "        # 2) input: merge your fields into one context string\n",
    "        alerts_str = \", \".join(alerts) if isinstance(alerts, list) else str(alerts)\n",
    "        history_str = \"\\n- \" + \"\\n- \".join(history) if isinstance(history, list) and len(history) else \"\"\n",
    "\n",
    "        input_text = (\n",
    "            f\"[[The claim is {claim}.]]\\n\"\n",
    "            f\"[[This is all the information known about the claim: {summary}.]]\\n\"\n",
    "            f\"[[These alerts point out omissions: {alerts_str}.]]\\n\"\n",
    "            f\"[[Take into account previous questions asked: {history_str}]]\"\n",
    "        )\n",
    "\n",
    "        # 3) output: your label\n",
    "        output_text = question\n",
    "\n",
    "        # final alpaca-formatted training text\n",
    "        text = alpaca_prompt.format(instruction, input_text, output_text) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "\n",
    "    return {\"text\": texts}\n",
    "\n",
    "dataset = ds.map(to_alpaca_text, batched=True, remove_columns=ds.column_names)\n",
    "\n",
    "# check one\n",
    "print(dataset[0][\"text\"][:4000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5019e7d",
   "metadata": {},
   "source": [
    "As mentioned in https://docs.unsloth.ai/get-started/fine-tuning-llms-guide/tutorial-how-to-finetune-llama-3-and-use-in-ollama:\n",
    "\n",
    "the rank is set to 32 to increase accuracy, but since the task is not that complicated and always the same, higher is not necessary. Lora_alpha is set to double this as recommended\n",
    "only the low rank matrices are trained so Bias is none\n",
    "Lora_dropout is a regularization parameter to prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73747460",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.11.3 patched 40 layers with 40 QKV layers, 40 O layers and 40 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# Add LoRA adapters\n",
    "target_modules = [\n",
    "    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\n",
    "    \"gate_proj\",\"up_proj\",\"down_proj\",\n",
    "]\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=32,\n",
    "    target_modules=target_modules,\n",
    "    lora_alpha=32*2,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a0c5f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "# test if bfloat16 is supported\n",
    "print(is_bfloat16_supported())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33eba51e",
   "metadata": {},
   "source": [
    "I spilt the data in training an test set to evaluate results, the setup of most of these parameters are derived from examples The instruct models are trained in 1 epoch and the base models in 2 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931471fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "from unsloth import unsloth_train, is_bfloat16_supported\n",
    "\n",
    "# split the dataset in a train and test set\n",
    "new_dataset = dataset.train_test_split(test_size = 0.01)\n",
    "\n",
    "# train the model\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    train_dataset = new_dataset[\"train\"],\n",
    "    eval_dataset = new_dataset[\"test\"],\n",
    "    tokenizer = tokenizer,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 1,\n",
    "    packing=False,\n",
    "    args= SFTConfig(\n",
    "        output_dir=\"outputs\",\n",
    "        seed=3407,\n",
    "\n",
    "        # Effective batch size = 2 * 4 = 8\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps = 5,\n",
    "\n",
    "        # Use epochs instead of max_steps\n",
    "        num_train_epochs=1,\n",
    "\n",
    "        # more precise tuning than default 2e-4\n",
    "        learning_rate=1e-4,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "\n",
    "        # bfloat16 is supported\n",
    "        bf16=False,\n",
    "\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.001,\n",
    "\n",
    "        logging_steps=10,\n",
    "        save_steps=200,\n",
    "        #save_total_limit=2,\n",
    "\n",
    "        # evaluate every 50 steps\n",
    "        per_device_eval_batch_size = 2,\n",
    "        eval_accumulation_steps = 4,\n",
    "        eval_strategy = \"steps\",\n",
    "        eval_steps = 25,\n",
    "    ),\n",
    ")\n",
    "\n",
    "#trainer.train()\n",
    "unsloth_train(trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32abe220",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8276505c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"The claim is 'Arizona officials caught changing ballots, have been arrested.'.\\nThis is all the information know about the claim: The claim asserts that Arizona officials were caught changing ballots and have been arrested. It is a qualitative, absolute statement with no cited source or methodological detail. Geography (Arizona) is specified, but the time frame is not, leading to alerts about the claim\\'s qualitative nature, missing source/methodology, and missing time period..\\n\"},\n",
    "]\n",
    "\n",
    "# Instruction for inference, matching the training instruction\n",
    "instruction_inference = \"Generate a reflective Socratic questions to challenge the student’s reasoning, surface blind spots, and encourage deeper reflection, rather than accepting the AI’s output at face value.Differentiate the question from previous questions asked about the claim.\"\n",
    "\n",
    "# Extract the user content\n",
    "user_content = messages[0][\"content\"]\n",
    "\n",
    "# Format the prompt using the alpaca_prompt template, leaving the response empty for generation\n",
    "input_prompt = alpaca_prompt.format(instruction_inference, user_content, \"\")\n",
    "\n",
    "# Tokenize the manually formatted prompt\n",
    "inputs = tokenizer(\n",
    "    input_prompt,\n",
    "    return_tensors=\"pt\",\n",
    "    return_attention_mask=True\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(input_ids = inputs.input_ids, attention_mask = inputs.attention_mask, streamer = text_streamer, max_new_tokens = 128, pad_token_id = tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779a7caa",
   "metadata": {},
   "source": [
    "Save the lora adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fff1af8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('lora_model\\\\tokenizer_config.json',\n",
       " 'lora_model\\\\special_tokens_map.json',\n",
       " 'lora_model\\\\chat_template.jinja',\n",
       " 'lora_model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"lora_model\")\n",
    "tokenizer.save_pretrained(\"lora_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d1dc1a",
   "metadata": {},
   "source": [
    "### Load model first before Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f16ab87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.11.3: Fast Mistral patching. Transformers: 4.57.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 5060 Ti. Num GPUs = 1. Max memory: 15.928 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.9.0+cu128. CUDA: 12.0. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Will load lora_model as a legacy tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What evidence would be needed to determine whether any Arizona officials have actually been arrested for altering ballots, and where might that evidence be found?</s>\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "if True: # Load model first before Inference (Set to False if it is already loaded)\n",
    "    from unsloth import FastLanguageModel\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = 2048,\n",
    "        dtype = None,\n",
    "        load_in_4bit = True,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "pass\n",
    "\n",
    "\n",
    "# Instruction for inference, matching the training instruction\n",
    "instruction_inference = \"Generate a reflective Socratic questions to challenge the student’s reasoning, surface blind spots, and encourage deeper reflection, rather than accepting the AI’s output at face value.Differentiate the question from previous questions asked about the claim.\"\n",
    "\n",
    "# Extract the user content\n",
    "user_content = messages[0][\"content\"]\n",
    "\n",
    "# Format the prompt using the alpaca_prompt template, leaving the response empty for generation\n",
    "input_prompt = alpaca_prompt.format(instruction_inference, user_content, \"\")\n",
    "\n",
    "inputs = tokenizer(\n",
    "    input_prompt,\n",
    "    return_tensors=\"pt\",\n",
    "    return_attention_mask=True\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(input_ids = inputs.input_ids, attention_mask = inputs.attention_mask, streamer = text_streamer, max_new_tokens = 128, pad_token_id = tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19191f76",
   "metadata": {},
   "source": [
    "### Exporting to Ollama\n",
    "\n",
    "Before you do this, you need to\n",
    "- install uv\n",
    "- download windows compatible files from https://github.com/ggml-org/llama.cpp/releases\n",
    "- copy: these files to llama.cpp\n",
    "- copy and rename the llama-quantize.exe files so Unsloth can find it (looks for unix style)\n",
    "copy llama-quantize.exe llama-quantize (from withing the llama.cpp folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1930e597",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\".env\", override=True)\n",
    "\n",
    "# Save to 8bit Q8_0\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
    "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
    "# And change hf to your username!\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
    "\n",
    "# Save to 16bit GGUF\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
    "\n",
    "# Save to q4_k_m GGUF\n",
    "if True: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
    "\n",
    "# Save to multiple GGUF options - much faster if you want multiple!\n",
    "if False:\n",
    "    model.push_to_hub_gguf(\n",
    "        \"hf/model\", # Change hf to your username!\n",
    "        tokenizer,\n",
    "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
    "        token = \"\", # Get a token at https://huggingface.co/settings/tokens\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d29694b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "subprocess.Popen([\"ollama\", \"serve\"])\n",
    "import time\n",
    "time.sleep(3) # Wait for a few seconds for Ollama to load!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca044116",
   "metadata": {},
   "source": [
    "If you work on collab, connect your drive to Colab and copy all the files to drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9605213",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ce99d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create base folder and subfolder using model_id\n",
    "!mkdir -p \"/content/drive/MyDrive/FineTunedModels/$model_id\"\n",
    "\n",
    "# Move generated artifacts into the model-specific folder\n",
    "!mv model       \"/content/drive/MyDrive/FineTunedModels/$model_id\"\n",
    "!mv outputs     \"/content/drive/MyDrive/FineTunedModels/$model_id\"\n",
    "!mv lora_model  \"/content/drive/MyDrive/FineTunedModels/$model_id\"\n",
    "!mv wandb       \"/content/drive/MyDrive/FineTunedModels/$model_id\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e77de5",
   "metadata": {},
   "source": [
    "Finally load the model in ollama. this step should be done on your local pc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e19ab45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components \u001b[K\u001b[?25h\u001b[?2026l\n",
      "Error: 400 Bad Request: invalid model name\n"
     ]
    }
   ],
   "source": [
    "!ollama create mistral7b-q4km -f Modelfile-mistral7b-q4km"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d601c6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "928e6933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME        ID              SIZE      MODIFIED     \n",
      "qwen3:4b    e55aed6fe643    2.5 GB    2 months ago    \n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
